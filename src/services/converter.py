"""
è½¬æ¢æœåŠ¡
å¤„ç†converté…ç½®ä¸­çš„é“¾æ¥ï¼Œä½¿ç”¨åŸæœ‰çš„è½¬æ¢é€»è¾‘ç”ŸæˆJSONè§„åˆ™é›†
èå…¥ç°æœ‰çš„ä¸‹è½½-å¤„ç†-ç¼–è¯‘æ¶æ„
"""

import os
import json
import pandas as pd
import re
import concurrent.futures
import requests
import yaml
import ipaddress
from io import StringIO
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple

from ..utils.config import ConfigManager
from ..utils.logger import Logger
from ..utils.file_utils import FileUtils
from ..utils.network import NetworkUtils


class ConvertedData:
    """è½¬æ¢æ•°æ®ç»“æœç±»"""
    
    def __init__(self, convert_name: str):
        self.convert_name = convert_name
        self.json_files: List[str] = []
        self.srs_files: List[str] = []
        self.success_count = 0
        self.total_count = 0
        self.errors: List[str] = []
    
    def add_converted_file(self, json_file: str, srs_file: str) -> None:
        """æ·»åŠ è½¬æ¢æˆåŠŸçš„æ–‡ä»¶"""
        self.json_files.append(json_file)
        self.srs_files.append(srs_file)
        self.success_count += 1
    
    def add_error(self, error: str) -> None:
        """æ·»åŠ é”™è¯¯ä¿¡æ¯"""
        self.errors.append(error)
    
    def set_total_count(self, count: int) -> None:
        """è®¾ç½®æ€»æ•°é‡"""
        self.total_count = count
    
    def is_successful(self) -> bool:
        """æ˜¯å¦æœ‰æˆåŠŸè½¬æ¢çš„æ•°æ®"""
        return self.success_count > 0


class ConverterService:
    """è½¬æ¢æœåŠ¡ç±»"""
    
    def __init__(self, config_manager: ConfigManager, logger: Logger, 
                 network_utils: NetworkUtils, file_utils: FileUtils):
        """
        åˆå§‹åŒ–è½¬æ¢æœåŠ¡
        
        Args:
            config_manager: é…ç½®ç®¡ç†å™¨
            logger: æ—¥å¿—è®°å½•å™¨
            network_utils: ç½‘ç»œå·¥å…·
            file_utils: æ–‡ä»¶å·¥å…·
        """
        self.config_manager = config_manager
        self.logger = logger
        self.network_utils = network_utils
        self.file_utils = file_utils
        
        # æ˜ å°„å­—å…¸ - ä»åŸæœ‰convert.pyç§»æ¤
        self.MAP_DICT = {
            'DOMAIN-SUFFIX': 'domain_suffix', 'HOST-SUFFIX': 'domain_suffix', 
            'host-suffix': 'domain_suffix', 'DOMAIN': 'domain', 'HOST': 'domain', 
            'host': 'domain', 'DOMAIN-KEYWORD': 'domain_keyword', 
            'HOST-KEYWORD': 'domain_keyword', 'host-keyword': 'domain_keyword', 
            'IP-CIDR': 'ip_cidr', 'ip-cidr': 'ip_cidr', 'IP-CIDR6': 'ip_cidr', 
            'IP6-CIDR': 'ip_cidr', 'SRC-IP-CIDR': 'source_ip_cidr', 
            'GEOIP': 'geoip', 'DST-PORT': 'port', 'SRC-PORT': 'source_port', 
            "URL-REGEX": "domain_regex", "DOMAIN-REGEX": "domain_regex"
        }
    
    def read_yaml_from_url(self, url: str) -> Any:
        """
        ä»URLè¯»å–YAMLæ•°æ®
        
        Args:
            url: YAMLæ–‡ä»¶URL
            
        Returns:
            è§£æåçš„YAMLæ•°æ®
        """
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        yaml_data = yaml.safe_load(response.text)
        return yaml_data
    
    def read_list_from_url(self, url: str) -> Tuple[Optional[pd.DataFrame], List[Dict]]:
        """
        ä»URLè¯»å–åˆ—è¡¨æ•°æ®
        
        Args:
            url: åˆ—è¡¨æ–‡ä»¶URL
            
        Returns:
            (DataFrameæ•°æ®, é€»è¾‘è§„åˆ™åˆ—è¡¨)
        """
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            return None, []
        
        csv_data = StringIO(response.text)
        df = pd.read_csv(csv_data, header=None, 
                        names=['pattern', 'address', 'other', 'other2', 'other3'], 
                        on_bad_lines='skip')
        
        filtered_rows = []
        rules = []
        
        # å¤„ç†é€»è¾‘è§„åˆ™
        if 'AND' in df['pattern'].values:
            and_rows = df[df['pattern'].str.contains('AND', na=False)]
            for _, row in and_rows.iterrows():
                rule = {
                    "type": "logical",
                    "mode": "and",
                    "rules": []
                }
                pattern = ",".join(row.values.astype(str))
                components = re.findall(r'\((.*?)\)', pattern)
                for component in components:
                    for keyword in self.MAP_DICT.keys():
                        if keyword in component:
                            match = re.search(f'{keyword},(.*)', component)
                            if match:
                                value = match.group(1)
                                rule["rules"].append({
                                    self.MAP_DICT[keyword]: value
                                })
                rules.append(rule)
        
        for index, row in df.iterrows():
            if 'AND' not in row['pattern']:
                filtered_rows.append(row)
        
        df_filtered = pd.DataFrame(filtered_rows, columns=['pattern', 'address', 'other', 'other2', 'other3'])
        return df_filtered, rules
    
    def is_ipv4_or_ipv6(self, address: str) -> Optional[str]:
        """
        æ£€æŸ¥åœ°å€æ˜¯å¦ä¸ºIPv4æˆ–IPv6
        
        Args:
            address: è¦æ£€æŸ¥çš„åœ°å€
            
        Returns:
            'ipv4', 'ipv6' æˆ– None
        """
        try:
            ipaddress.IPv4Network(address)
            return 'ipv4'
        except ValueError:
            try:
                ipaddress.IPv6Network(address)
                return 'ipv6'
            except ValueError:
                return None
    
    def parse_and_convert_to_dataframe(self, link: str) -> Tuple[Optional[pd.DataFrame], List[Dict]]:
        """
        è§£æé“¾æ¥å¹¶è½¬æ¢ä¸ºDataFrame
        
        Args:
            link: è¦è§£æçš„é“¾æ¥
            
        Returns:
            (DataFrameæ•°æ®, é€»è¾‘è§„åˆ™åˆ—è¡¨)
        """
        rules = []
        
        # æ ¹æ®é“¾æ¥æ‰©å±•ååˆ†æƒ…å†µå¤„ç†
        if link.endswith('.yaml') or link.endswith('.txt'):
            try:
                yaml_data = self.read_yaml_from_url(link)
                rows = []
                if not isinstance(yaml_data, str):
                    items = yaml_data.get('payload', [])
                else:
                    lines = yaml_data.splitlines()
                    line_content = lines[0]
                    items = line_content.split()
                
                for item in items:
                    address = item.strip("'")
                    if ',' not in item:
                        if self.is_ipv4_or_ipv6(item):
                            pattern = 'IP-CIDR'
                        else:
                            if address.startswith('+') or address.startswith('.'):
                                pattern = 'DOMAIN-SUFFIX'
                                address = address[1:]
                                if address.startswith('.'):
                                    address = address[1:]
                            else:
                                pattern = 'DOMAIN'
                    else:
                        pattern, address = item.split(',', 1)
                    
                    if ',' in address:
                        address = address.split(',', 1)[0]
                    
                    rows.append({'pattern': pattern.strip(), 'address': address.strip(), 'other': None})
                
                df = pd.DataFrame(rows, columns=['pattern', 'address', 'other'])
            except:
                df, rules = self.read_list_from_url(link)
        else:
            df, rules = self.read_list_from_url(link)
        
        return df, rules
    
    def sort_dict(self, obj: Any) -> Any:
        """
        å¯¹å­—å…¸è¿›è¡Œæ’åºï¼Œå«list of dict
        
        Args:
            obj: è¦æ’åºçš„å¯¹è±¡
            
        Returns:
            æ’åºåçš„å¯¹è±¡
        """
        if isinstance(obj, dict):
            return {k: self.sort_dict(obj[k]) for k in sorted(obj)}
        elif isinstance(obj, list) and all(isinstance(elem, dict) for elem in obj):
            return sorted([self.sort_dict(x) for x in obj], key=lambda d: sorted(d.keys())[0])
        elif isinstance(obj, list):
            return sorted(self.sort_dict(x) for x in obj)
        else:
            return obj
    
    def convert_single_link(self, link: str, output_directory: Path) -> Optional[str]:
        """
        è½¬æ¢å•ä¸ªé“¾æ¥ä¸ºJSONå’ŒSRSæ–‡ä»¶
        
        Args:
            link: è¦è½¬æ¢çš„é“¾æ¥
            output_directory: è¾“å‡ºç›®å½•
            
        Returns:
            ç”Ÿæˆçš„JSONæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            self.logger.info(f"ğŸ”„ è½¬æ¢é“¾æ¥: {link}")
            
            # è§£æé“¾æ¥æ•°æ®
            with concurrent.futures.ThreadPoolExecutor() as executor:
                results = list(executor.map(self.parse_and_convert_to_dataframe, [link]))
                dfs = [df for df, rules in results]
                rules_list = [rules for df, rules in results]
                df = pd.concat(dfs, ignore_index=True)
            
            # æ•°æ®æ¸…ç†
            df = df[~df['pattern'].str.contains('#')].reset_index(drop=True)
            df = df[df['pattern'].isin(self.MAP_DICT.keys())].reset_index(drop=True)
            df = df.drop_duplicates().reset_index(drop=True)
            df['pattern'] = df['pattern'].replace(self.MAP_DICT)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            self.file_utils.ensure_dir(output_directory)
            
            # æ„å»ºè§„åˆ™é›†
            result_rules = {"version": 2, "rules": []}
            domain_entries = []
            
            for pattern, addresses in df.groupby('pattern')['address'].apply(list).to_dict().items():
                if pattern == 'domain_suffix':
                    rule_entry = {pattern: [address.strip() for address in addresses]}
                    result_rules["rules"].append(rule_entry)
                elif pattern == 'domain':
                    domain_entries.extend([address.strip() for address in addresses])
                else:
                    rule_entry = {pattern: [address.strip() for address in addresses]}
                    result_rules["rules"].append(rule_entry)
            
            # åˆ é™¤domain_entriesä¸­çš„é‡å¤å€¼
            domain_entries = list(set(domain_entries))
            if domain_entries:
                result_rules["rules"].insert(0, {'domain': domain_entries})
            
            # ç”Ÿæˆæ–‡ä»¶å
            file_name = output_directory / f"{os.path.basename(link).split('.')[0]}.json"
            
            # å†™å…¥JSONæ–‡ä»¶
            with open(file_name, 'w', encoding='utf-8') as output_file:
                result_rules_str = json.dumps(self.sort_dict(result_rules), ensure_ascii=False, indent=2)
                result_rules_str = result_rules_str.replace('\\\\', '\\')
                output_file.write(result_rules_str)
            
            self.logger.success(f"âœ… è½¬æ¢å®Œæˆ: {file_name}")
            return str(file_name)
                
        except Exception as e:
            self.logger.error(f"âŒ è½¬æ¢é“¾æ¥å¤±è´¥: {link} - {str(e)}")
            return None
    
    def convert_ruleset(self, convert_name: str, urls: List[str]) -> ConvertedData:
        """
        è½¬æ¢å•ä¸ªconvertè§„åˆ™é›†çš„æ‰€æœ‰é“¾æ¥
        
        Args:
            convert_name: convertè§„åˆ™é›†åç§°
            urls: URLåˆ—è¡¨
            
        Returns:
            è½¬æ¢æ•°æ®ç»“æœ
        """
        self.logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢è§„åˆ™é›†: {convert_name}")
        self.logger.info(f"ğŸ“‹ é“¾æ¥æ•°é‡: {len(urls)}")
        
        # åˆ›å»ºè½¬æ¢ç»“æœå¯¹è±¡
        converted_data = ConvertedData(convert_name)
        converted_data.set_total_count(len(urls))
        
        # è·å–è¾“å‡ºç›®å½•é…ç½®
        output_config = self.config_manager.get_output_config()
        json_dir = Path(output_config["json_dir"])
        
        # ä¸ºæ¯ä¸ªconvertè§„åˆ™é›†åˆ›å»ºå­ç›®å½•
        convert_json_dir = json_dir / "convert" / convert_name
        
        # è½¬æ¢æ¯ä¸ªé“¾æ¥
        for i, url in enumerate(urls, 1):
            self.logger.info(f"ğŸ”„ è½¬æ¢é“¾æ¥ ({i}/{len(urls)}): {url}")
            
            json_file = self.convert_single_link(url, convert_json_dir)
            if json_file:
                converted_data.add_converted_file(json_file, "")  # SRSæ–‡ä»¶å°†åœ¨ç¼–è¯‘é˜¶æ®µç»Ÿä¸€ç”Ÿæˆ
            else:
                converted_data.add_error(f"è½¬æ¢å¤±è´¥: {url}")
        
        # è¾“å‡ºè½¬æ¢ç»“æœæ‘˜è¦
        if converted_data.is_successful():
            self.logger.success(f"âœ… è§„åˆ™é›† {convert_name} è½¬æ¢å®Œæˆ")
            self.logger.info(f"ğŸ“Š æˆåŠŸ: {converted_data.success_count}/{converted_data.total_count}")
            self.logger.info(f"ğŸ“„ JSONæ–‡ä»¶: {len(converted_data.json_files)} ä¸ª")
        else:
            self.logger.error(f"âŒ è§„åˆ™é›† {convert_name} è½¬æ¢å¤±è´¥")
        
        # è¾“å‡ºé”™è¯¯ä¿¡æ¯
        for error in converted_data.errors:
            self.logger.warning(f"âš ï¸ {error}")
        
        return converted_data
    
    def convert_all_rulesets(self) -> Dict[str, ConvertedData]:
        """
        è½¬æ¢æ‰€æœ‰converté…ç½®ä¸­çš„è§„åˆ™é›†
        
        Returns:
            convertè§„åˆ™é›†åç§°åˆ°è½¬æ¢æ•°æ®çš„æ˜ å°„
        """
        # è·å–converté…ç½®
        config = self.config_manager.load_config()
        convert_config = config.get('convert', {})
        
        if not convert_config:
            self.logger.info("ğŸ“‹ æ²¡æœ‰å‘ç°converté…ç½®ï¼Œè·³è¿‡è½¬æ¢é˜¶æ®µ")
            return {}
        
        results = {}
        
        self.logger.header("å¼€å§‹è½¬æ¢é˜¶æ®µ")
        self.logger.info(f"ğŸ“‹ å‘ç° {len(convert_config)} ä¸ªconvertè§„åˆ™é›†")
        
        for i, (convert_name, urls) in enumerate(convert_config.items(), 1):
            self.logger.step(f"è½¬æ¢è§„åˆ™é›†: {convert_name}", i, len(convert_config))
            
            try:
                converted_data = self.convert_ruleset(convert_name, urls)
                results[convert_name] = converted_data
                
            except Exception as e:
                self.logger.error(f"âŒ è§„åˆ™é›† {convert_name} è½¬æ¢å¼‚å¸¸: {str(e)}")
                # åˆ›å»ºå¤±è´¥çš„è½¬æ¢æ•°æ®
                failed_data = ConvertedData(convert_name)
                failed_data.set_total_count(len(urls))
                failed_data.add_error(f"è½¬æ¢å¼‚å¸¸: {str(e)}")
                results[convert_name] = failed_data
            
            # æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if i < len(convert_config):
                self.logger.info("â”€" * 50)
        
        # è¾“å‡ºæ€»ä½“ç»Ÿè®¡
        successful_converts = sum(1 for data in results.values() if data.is_successful())
        self.logger.separator("è½¬æ¢é˜¶æ®µå®Œæˆ")
        self.logger.success(f"âœ… è½¬æ¢å®Œæˆ: {successful_converts}/{len(convert_config)} ä¸ªè§„åˆ™é›†æˆåŠŸ")
        
        return results
    
    def get_convert_statistics(self, results: Dict[str, ConvertedData]) -> Dict[str, Any]:
        """
        è·å–è½¬æ¢ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            results: è½¬æ¢ç»“æœå­—å…¸
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total_converts = len(results)
        successful_converts = sum(1 for data in results.values() if data.is_successful())
        total_links = sum(data.total_count for data in results.values())
        successful_links = sum(data.success_count for data in results.values())
        
        total_json_files = sum(len(data.json_files) for data in results.values())
        return {
            'total_converts': total_converts,
            'successful_converts': successful_converts,
            'total_links': total_links,
            'successful_links': successful_links,
            'total_json_files': total_json_files,
            'success_rate': (successful_links / total_links * 100) if total_links > 0 else 0
        }