"""
è½¬æ¢æœåŠ¡
å¤„ç†converté…ç½®ä¸­çš„é“¾æ¥ï¼Œä½¿ç”¨åŸæœ‰çš„è½¬æ¢é€»è¾‘ç”ŸæˆJSONè§„åˆ™é›†
èå…¥ç°æœ‰çš„ä¸‹è½½-å¤„ç†-ç¼–è¯‘æ¶æ„
"""

import json
import pandas as pd
import re
import yaml
import ipaddress
from io import StringIO
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple

from ..utils.config import ConfigManager
from ..utils.logger import Logger
from ..utils.file_utils import FileUtils
from ..utils.network import NetworkUtils
from .downloader import DownloadedData


class ConvertedData:
    """è½¬æ¢æ•°æ®ç»“æœç±»"""
    
    def __init__(self, convert_name: str):
        self.convert_name = convert_name
        self.json_files: List[str] = []
        self.srs_files: List[str] = []
        self.success_count = 0
        self.total_count = 0
        self.errors: List[str] = []
    
    def add_converted_file(self, json_file: str, srs_file: str) -> None:
        """æ·»åŠ è½¬æ¢æˆåŠŸçš„æ–‡ä»¶"""
        self.json_files.append(json_file)
        self.srs_files.append(srs_file)
        self.success_count += 1
    
    def add_error(self, error: str) -> None:
        """æ·»åŠ é”™è¯¯ä¿¡æ¯"""
        self.errors.append(error)
    
    def set_total_count(self, count: int) -> None:
        """è®¾ç½®æ€»æ•°é‡"""
        self.total_count = count
    
    def is_successful(self) -> bool:
        """æ˜¯å¦æœ‰æˆåŠŸè½¬æ¢çš„æ•°æ®"""
        return self.success_count > 0


class ConverterService:
    """è½¬æ¢æœåŠ¡ç±»"""
    
    def __init__(self, config_manager: ConfigManager, logger: Logger, 
                 network_utils: NetworkUtils, file_utils: FileUtils):
        """
        åˆå§‹åŒ–è½¬æ¢æœåŠ¡
        
        Args:
            config_manager: é…ç½®ç®¡ç†å™¨
            logger: æ—¥å¿—è®°å½•å™¨
            network_utils: ç½‘ç»œå·¥å…·
            file_utils: æ–‡ä»¶å·¥å…·
        """
        self.config_manager = config_manager
        self.logger = logger
        self.network_utils = network_utils
        self.file_utils = file_utils

        
        # æ˜ å°„å­—å…¸ - ä»åŸæœ‰convert.pyç§»æ¤
        self.MAP_DICT = {
            'DOMAIN-SUFFIX': 'domain_suffix', 'HOST-SUFFIX': 'domain_suffix', 
            'host-suffix': 'domain_suffix', 'DOMAIN': 'domain', 'HOST': 'domain', 
            'host': 'domain', 'DOMAIN-KEYWORD': 'domain_keyword', 
            'HOST-KEYWORD': 'domain_keyword', 'host-keyword': 'domain_keyword', 
            'IP-CIDR': 'ip_cidr', 'ip-cidr': 'ip_cidr', 'IP-CIDR6': 'ip_cidr', 
            'IP6-CIDR': 'ip_cidr', 'SRC-IP-CIDR': 'source_ip_cidr', 
            'GEOIP': 'geoip', 'DST-PORT': 'port', 'SRC-PORT': 'source_port', 
            "URL-REGEX": "domain_regex", "DOMAIN-REGEX": "domain_regex"
        }
    

    
    def convert_all_rulesets_from_downloads(self, convert_download_results: Dict[str, DownloadedData]) -> Dict[str, ConvertedData]:
        """
        ä»å·²ä¸‹è½½çš„æ•°æ®è½¬æ¢convertè§„åˆ™é›†
        
        Args:
            convert_download_results: convertç±»å‹çš„ä¸‹è½½ç»“æœ
            
        Returns:
            convertè§„åˆ™é›†åç§°åˆ°è½¬æ¢æ•°æ®çš„æ˜ å°„
        """
        if not convert_download_results:
            self.logger.info("ğŸ“‹ æ²¡æœ‰convertä¸‹è½½æ•°æ®ï¼Œè·³è¿‡è½¬æ¢")
            return {}
        
        results = {}
        
        self.logger.header("å¼€å§‹convertè½¬æ¢é˜¶æ®µ")
        self.logger.info(f"ğŸ“‹ å¤„ç† {len(convert_download_results)} ä¸ªconvertè§„åˆ™é›†")
        
        for i, (convert_name, download_data) in enumerate(convert_download_results.items(), 1):
            self.logger.step(f"è½¬æ¢è§„åˆ™é›†: {convert_name}", i, len(convert_download_results))
            
            try:
                converted_data = self.convert_ruleset_from_download(convert_name, download_data)
                results[convert_name] = converted_data
                
            except Exception as e:
                self.logger.error(f"âŒ è§„åˆ™é›† {convert_name} è½¬æ¢å¼‚å¸¸: {str(e)}")
                # åˆ›å»ºå¤±è´¥çš„è½¬æ¢æ•°æ®
                failed_data = ConvertedData(convert_name)
                failed_data.set_total_count(download_data.total_count)
                failed_data.add_error(f"è½¬æ¢å¼‚å¸¸: {str(e)}")
                results[convert_name] = failed_data
            
            # æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if i < len(convert_download_results):
                self.logger.info("â”€" * 50)
        
        # è¾“å‡ºæ€»ä½“ç»Ÿè®¡
        stats = self.get_convert_statistics(results)
        self.logger.separator("convertè½¬æ¢é˜¶æ®µå®Œæˆ")
        self.logger.success(f"âœ… è½¬æ¢å®Œæˆ: {stats['successful_urls']}/{stats['total_urls']} ä¸ªé…ç½®æˆåŠŸ")
        
        return results
    
    def convert_ruleset_from_download(self, convert_name: str, download_data: DownloadedData) -> ConvertedData:
        """
        ä»å·²ä¸‹è½½çš„æ•°æ®è½¬æ¢å•ä¸ªconvertè§„åˆ™é›†
        
        Args:
            convert_name: convertè§„åˆ™é›†åç§°
            download_data: å·²ä¸‹è½½çš„æ•°æ®
            
        Returns:
            è½¬æ¢æ•°æ®ç»“æœ
        """
        self.logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢è§„åˆ™é›†: {convert_name}")
        self.logger.info(f"ğŸ“‹ å·²ä¸‹è½½æ–‡ä»¶æ•°é‡: {len(download_data.text_files)}")
        
        # åˆ›å»ºè½¬æ¢ç»“æœå¯¹è±¡
        converted_data = ConvertedData(convert_name)
        converted_data.set_total_count(download_data.total_count)
        
        # è·å–è¾“å‡ºç›®å½•é…ç½®
        output_config = self.config_manager.get_output_config()
        json_dir = Path(output_config["json_dir"])
        self.file_utils.ensure_dir(json_dir)
        
        # åˆå§‹åŒ–åˆå¹¶ç»“æ„
        merged_by_type = {}  # pattern -> set of stripped addresses (for dedup)
        all_logic_rules = []  # æ”¶é›†æ‰€æœ‰é€»è¾‘è§„åˆ™
        domain_entries = set()  # å•ç‹¬æ”¶é›†domainï¼Œç”¨äºå»é‡å¹¶æ’å…¥å¼€å¤´
        
        # å¤„ç†å·²ä¸‹è½½çš„æ–‡æœ¬æ–‡ä»¶
        for i, file_path in enumerate(download_data.text_files, 1):
            self.logger.info(f"ğŸ”„ å¤„ç†æ–‡ä»¶ ({i}/{len(download_data.text_files)}): {file_path}")
            
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è§£æ
                df, logic_rules = self.parse_downloaded_file(file_path)
                
                # æ”¶é›†é€»è¾‘è§„åˆ™
                all_logic_rules.extend(logic_rules)
                
                # è¿‡æ»¤df
                filtered_rows = []
                for index, row in df.iterrows():
                    if 'AND' not in str(row.get('pattern', '')):
                        filtered_rows.append(row)
                
                if filtered_rows:
                    df_filtered = pd.DataFrame(filtered_rows)
                    
                    # groupbyå¹¶åˆå¹¶åˆ°merged_by_type
                    if 'pattern' in df_filtered.columns and 'address' in df_filtered.columns:
                        for pattern, addresses in df_filtered.groupby('pattern')['address'].apply(list).to_dict().items():
                            stripped = {str(addr).strip() for addr in addresses}  # set for dedup
                            mapped_pattern = self.MAP_DICT.get(pattern, pattern)  # æ˜ å°„åˆ°æ ‡å‡†ç±»å‹
                            
                            if mapped_pattern == 'domain':
                                domain_entries.update(stripped)
                            else:
                                if mapped_pattern not in merged_by_type:
                                    merged_by_type[mapped_pattern] = set()
                                merged_by_type[mapped_pattern].update(stripped)
                
                converted_data.success_count += 1
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ–‡ä»¶å¤„ç†å¤±è´¥: {file_path} - {str(e)}")
                converted_data.add_error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {file_path} - {str(e)}")
                continue
        
        # å¦‚æœæœ‰æˆåŠŸå¤„ç†çš„æ–‡ä»¶ï¼Œæ„å»ºåˆå¹¶çš„è§„åˆ™é›†
        if merged_by_type or all_logic_rules or domain_entries:
            merged_ruleset = {"version": self.config_manager.get_version(), "rules": []}
            
            # æ·»åŠ édomainè§„åˆ™
            for pattern, values in merged_by_type.items():
                if values:
                    sorted_values = sorted(list(values))  # å¯é€‰ï¼šæ’åº
                    merged_ruleset["rules"].append({pattern: sorted_values})
            
            # æ·»åŠ domainï¼ˆæ’å…¥å¼€å¤´ï¼Œå»é‡ï¼‰
            if domain_entries:
                sorted_domains = sorted(list(domain_entries))
                merged_ruleset["rules"].insert(0, {'domain': sorted_domains})
            
            # æ·»åŠ é€»è¾‘è§„åˆ™ï¼ˆè¿½åŠ åˆ°æœ«å°¾ï¼‰
            merged_ruleset["rules"].extend(all_logic_rules)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨convert_nameï¼‰
            file_name = json_dir / f"{convert_name}.json"
            
            # å†™å…¥JSONæ–‡ä»¶
            with open(file_name, 'w', encoding='utf-8') as output_file:
                result_rules_str = json.dumps(self.sort_dict(merged_ruleset), ensure_ascii=False, indent=2)
                result_rules_str = result_rules_str.replace('\\\\', '\\')
                output_file.write(result_rules_str)
            
            converted_data.add_converted_file(str(file_name), "")
            self.logger.info(f"âœ… è½¬æ¢å®Œæˆï¼ˆåˆå¹¶åˆ°å•ä¸ªæ–‡ä»¶ï¼‰: {file_name}")
        else:
            self.logger.error(f"âŒ è§„åˆ™é›† {convert_name} æ— æœ‰æ•ˆæ•°æ®")
        
        # è¾“å‡ºè½¬æ¢ç»“æœæ‘˜è¦
        if converted_data.is_successful():
            self.logger.info(f"âœ… è§„åˆ™é›† {convert_name} è½¬æ¢å®Œæˆ")
            self.logger.info(f"ğŸ“Š æˆåŠŸ: {converted_data.success_count}/{converted_data.total_count}")
            self.logger.info(f"ğŸ“„ JSONæ–‡ä»¶: {len(converted_data.json_files)} ä¸ª")
        else:
            self.logger.error(f"âŒ è§„åˆ™é›† {convert_name} è½¬æ¢å¤±è´¥")
        
        # è¾“å‡ºé”™è¯¯ä¿¡æ¯
        for error in converted_data.errors:
            self.logger.warning(f"âš ï¸ {error}")
        
        return converted_data
    
    def parse_downloaded_file(self, file_path: str) -> Tuple[pd.DataFrame, List[Dict]]:
        """
        è§£æå·²ä¸‹è½½çš„æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            (DataFrameæ•°æ®, é€»è¾‘è§„åˆ™åˆ—è¡¨)
        """
        # è¯»å–æ–‡ä»¶å†…å®¹
        content_lines = self.file_utils.read_text_file(file_path)
        
        # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶è§£æ
        if file_path.endswith('.yaml') or file_path.endswith('.yml'):
            # YAMLæ–‡ä»¶å¤„ç†
            yaml_content = '\n'.join(content_lines)
            yaml_data = yaml.safe_load(yaml_content)
            
            rows = []
            if not isinstance(yaml_data, str):
                items = yaml_data.get('payload', []) if isinstance(yaml_data, dict) else yaml_data
            else:
                items = yaml_data.splitlines()
            
            for item in items:
                if isinstance(item, str):
                    # ç®€å•å¤„ç†ï¼Œå‡è®¾æ¯è¡Œæ˜¯ä¸€ä¸ªpattern:addresså¯¹
                    parts = item.split(',', 1)
                    if len(parts) == 2:
                        rows.append({'pattern': parts[0].strip(), 'address': parts[1].strip()})
                    else:
                        rows.append({'pattern': 'domain', 'address': item.strip()})
                elif isinstance(item, dict):
                    for key, value in item.items():
                        if isinstance(value, list):
                            for v in value:
                                rows.append({'pattern': key, 'address': v})
                        else:
                            rows.append({'pattern': key, 'address': value})
            
            df = pd.DataFrame(rows)
            return df, []  # YAMLé€šå¸¸æ²¡æœ‰é€»è¾‘è§„åˆ™
        else:
            # æ–‡æœ¬æ–‡ä»¶å¤„ç†ï¼ˆCSVæ ¼å¼ï¼‰
            csv_content = StringIO('\n'.join(content_lines))
            df = pd.read_csv(csv_content, header=None, 
                            names=['pattern', 'address', 'other', 'other2', 'other3'], 
                            on_bad_lines='skip')
            
            filtered_rows = []
            rules = []
            
            # å¤„ç†é€»è¾‘è§„åˆ™
            if 'AND' in df['pattern'].values:
                and_rows = df[df['pattern'].str.contains('AND', na=False)]
                for _, row in and_rows.iterrows():
                    rule = {
                        "type": "logical",
                        "mode": "and",
                        "rules": []
                    }
                    pattern = ",".join(row.values.astype(str))
                    components = re.findall(r'\((.*?)\)', pattern)
                    for component in components:
                        for keyword in self.MAP_DICT.keys():
                            if keyword in component:
                                match = re.search(f'{keyword},(.*)', component)
                                if match:
                                    value = match.group(1)
                                    rule["rules"].append({
                                        self.MAP_DICT[keyword]: value
                                    })
                    rules.append(rule)
            
            for index, row in df.iterrows():
                if 'AND' not in str(row['pattern']):
                    filtered_rows.append(row)
            
            df_filtered = pd.DataFrame(filtered_rows, columns=['pattern', 'address', 'other', 'other2', 'other3'])
            return df_filtered, rules
    
    def get_convert_statistics(self, results: Dict[str, ConvertedData]) -> Dict[str, Any]:
        """
        è·å–è½¬æ¢ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            results: è½¬æ¢ç»“æœå­—å…¸
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total_converts = len(results)
        successful_converts = sum(1 for data in results.values() if data.is_successful())
        total_urls = sum(data.total_count for data in results.values())
        successful_urls = sum(data.success_count for data in results.values())
        total_json_files = sum(len(data.json_files) for data in results.values())
        
        return {
            'total_converts': total_converts,
            'successful_converts': successful_converts,
            'total_urls': total_urls,
            'successful_urls': successful_urls,
            'total_json_files': total_json_files,
            'success_rate': (successful_urls / total_urls * 100) if total_urls > 0 else 0
        }

    def sort_dict(self, data: Dict) -> Dict:
        """
        é€’å½’æ’åºå­—å…¸ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        
        Args:
            data: è¦æ’åºçš„å­—å…¸æˆ–æ•°æ®
            
        Returns:
            æ’åºåçš„å­—å…¸
        """
        if isinstance(data, dict):
            return {k: self.sort_dict(v) for k, v in sorted(data.items())}
        elif isinstance(data, list):
            return [self.sort_dict(item) if isinstance(item, (dict, list)) else item for item in data]
        else:
            return data