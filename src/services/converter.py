"""
è½¬æ¢æœåŠ¡
å¤„ç†converté…ç½®ä¸­çš„é“¾æ¥ï¼Œä½¿ç”¨åŸæœ‰çš„è½¬æ¢é€»è¾‘ç”ŸæˆJSONè§„åˆ™é›†
èå…¥ç°æœ‰çš„ä¸‹è½½-å¤„ç†-ç¼–è¯‘æ¶æ„
"""

import json
import pandas as pd
import re
import yaml
from io import StringIO
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple

from ..utils.config import ConfigManager
from ..utils.logger import Logger
from ..utils.file_utils import FileUtils
from ..utils.network import NetworkUtils
from .downloader import DownloadedData





class ConverterService:
    """è½¬æ¢æœåŠ¡ç±»"""
    
    def __init__(self, config_manager: ConfigManager, logger: Logger, 
                 network_utils: NetworkUtils, file_utils: FileUtils):
        """
        åˆå§‹åŒ–è½¬æ¢æœåŠ¡
        
        Args:
            config_manager: é…ç½®ç®¡ç†å™¨
            logger: æ—¥å¿—è®°å½•å™¨
            network_utils: ç½‘ç»œå·¥å…·
            file_utils: æ–‡ä»¶å·¥å…·
        """
        self.config_manager = config_manager
        self.logger = logger
        self.network_utils = network_utils
        self.file_utils = file_utils

        
        # æ˜ å°„å­—å…¸ - ä»åŸæœ‰convert.pyç§»æ¤
        self.MAP_DICT = {
            'DOMAIN-SUFFIX': 'domain_suffix', 'HOST-SUFFIX': 'domain_suffix', 
            'host-suffix': 'domain_suffix', 'DOMAIN': 'domain', 'HOST': 'domain', 
            'host': 'domain', 'DOMAIN-KEYWORD': 'domain_keyword', 
            'HOST-KEYWORD': 'domain_keyword', 'host-keyword': 'domain_keyword', 
            'IP-CIDR': 'ip_cidr', 'ip-cidr': 'ip_cidr', 'IP-CIDR6': 'ip_cidr', 
            'IP6-CIDR': 'ip_cidr', 'SRC-IP-CIDR': 'source_ip_cidr', 
            'GEOIP': 'geoip', 'DST-PORT': 'port', 'SRC-PORT': 'source_port', 
            "URL-REGEX": "domain_regex", "DOMAIN-REGEX": "domain_regex"
        }
    

    

    

    

    

    
    def process_convert_data(self, convert_download_results: Dict[str, DownloadedData]) -> Dict[str, Any]:
        """
        å¤„ç†å·²ä¸‹è½½çš„convertæ•°æ®ï¼Œè½¬æ¢ä¸ºJSONæ ¼å¼å¹¶è¿”å›ProcessedDataæ ¼å¼
        
        Args:
            convert_download_results: convertæ•°æ®çš„ä¸‹è½½ç»“æœ
            
        Returns:
            è½¬æ¢åçš„å¤„ç†ç»“æœï¼ˆæ ¼å¼ä¸ProcessedDataå…¼å®¹ï¼‰
        """
        from .processor import ProcessedData  # é¿å…å¾ªç¯å¯¼å…¥
        
        if not convert_download_results:
            self.logger.info("ğŸ“‹ æ²¡æœ‰convertæ•°æ®éœ€è¦å¤„ç†")
            return {}
        
        results = {}
        
        self.logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†convertæ•°æ®: {len(convert_download_results)} ä¸ªè½¬æ¢æº")
        
        for convert_name, download_data in convert_download_results.items():
            self.logger.info(f"ğŸ”„ å¤„ç†è½¬æ¢æº: {convert_name}")
            
            try:
                if not download_data.is_successful():
                    # åˆ›å»ºå¤±è´¥çš„å¤„ç†ç»“æœ
                    failed_result = ProcessedData(convert_name, "", 0, False, f"ä¸‹è½½å¤±è´¥: {', '.join(download_data.errors)}")
                    results[convert_name] = failed_result
                    continue
                
                # è·å–è¾“å‡ºç›®å½•é…ç½®
                output_config = self.config_manager.get_output_config()
                json_dir = Path(output_config["json_dir"])
                self.file_utils.ensure_dir(json_dir)
                
                # åˆå§‹åŒ–åˆå¹¶ç»“æ„
                merged_by_type = {}
                all_logic_rules = []
                domain_entries = set()
                
                # å¤„ç†ä¸‹è½½çš„æ–‡æœ¬æ–‡ä»¶
                for text_file in download_data.text_files:
                    try:
                        # æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­å¤„ç†æ–¹å¼
                        if text_file.endswith('.yaml') or text_file.endswith('.yml'):
                            yaml_content = self.file_utils.read_text_file(text_file)
                            yaml_data = yaml.safe_load('\n'.join(yaml_content))
                            df, logic_rules = self._process_yaml_data(yaml_data)
                        else:
                            # å¤„ç†ä¸ºåˆ—è¡¨æ–‡ä»¶
                            df, logic_rules = self._process_text_file(text_file)
                        
                        # æ”¶é›†é€»è¾‘è§„åˆ™
                        all_logic_rules.extend(logic_rules)
                        
                        # åˆå¹¶è§„åˆ™
                        self._merge_rules_to_dict(df, merged_by_type, domain_entries)
                        
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ å¤„ç†æ–‡ä»¶å¤±è´¥: {text_file} - {str(e)}")
                        continue
                
                # æ„å»ºåˆå¹¶çš„è§„åˆ™é›†
                if merged_by_type or all_logic_rules or domain_entries:
                    merged_ruleset = {"version": self.config_manager.get_version(), "rules": []}
                    
                    # æ·»åŠ édomainè§„åˆ™
                    for pattern, values in merged_by_type.items():
                        if values:
                            sorted_values = sorted(list(values))
                            merged_ruleset["rules"].append({pattern: sorted_values})
                    
                    # æ·»åŠ domainï¼ˆæ’å…¥å¼€å¤´ï¼‰
                    if domain_entries:
                        sorted_domains = sorted(list(domain_entries))
                        merged_ruleset["rules"].insert(0, {'domain': sorted_domains})
                    
                    # æ·»åŠ é€»è¾‘è§„åˆ™
                    merged_ruleset["rules"].extend(all_logic_rules)
                    
                    # ç”ŸæˆJSONæ–‡ä»¶
                    json_file = json_dir / f"{convert_name}.json"
                    with open(json_file, 'w', encoding='utf-8') as output_file:
                        result_rules_str = json.dumps(self.sort_dict(merged_ruleset), ensure_ascii=False, indent=2)
                        result_rules_str = result_rules_str.replace('\\\\', '\\')
                        output_file.write(result_rules_str)
                    
                    # ç»Ÿè®¡è§„åˆ™æ•°é‡
                    rule_count = sum(len(rule_dict) for rule in merged_ruleset["rules"] for rule_dict in [rule] if isinstance(rule_dict, dict))
                    
                    # åˆ›å»ºæˆåŠŸçš„å¤„ç†ç»“æœ
                    success_result = ProcessedData(convert_name, str(json_file), rule_count, True, None)
                    results[convert_name] = success_result
                    
                    self.logger.info(f"âœ… è½¬æ¢æº {convert_name} å¤„ç†å®Œæˆ: {rule_count} æ¡è§„åˆ™")
                else:
                    # åˆ›å»ºå¤±è´¥çš„å¤„ç†ç»“æœ
                    failed_result = ProcessedData(convert_name, "", 0, False, "æ²¡æœ‰æœ‰æ•ˆçš„è§„åˆ™æ•°æ®")
                    results[convert_name] = failed_result
                    self.logger.warning(f"âš ï¸ è½¬æ¢æº {convert_name} æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                
            except Exception as e:
                self.logger.error(f"âŒ è½¬æ¢æº {convert_name} å¤„ç†å¼‚å¸¸: {str(e)}")
                failed_result = ProcessedData(convert_name, "", 0, False, f"å¤„ç†å¼‚å¸¸: {str(e)}")
                results[convert_name] = failed_result
        
        # è¾“å‡ºç»Ÿè®¡
        successful_converts = sum(1 for data in results.values() if data.success)
        self.logger.info(f"âœ… convertæ•°æ®å¤„ç†å®Œæˆ: {successful_converts}/{len(convert_download_results)} ä¸ªè½¬æ¢æºæˆåŠŸ")
        
        return results
    
    def _process_yaml_data(self, yaml_data: Any) -> Tuple[pd.DataFrame, List[Dict]]:
        """å¤„ç†YAMLæ•°æ®"""
        rows = []
        if not isinstance(yaml_data, str):
            items = yaml_data.get('payload', [])
        else:
            items = yaml_data.splitlines()
        
        for item in items:
            if isinstance(item, str):
                parts = item.split(',', 1)
                if len(parts) == 2:
                    rows.append({'pattern': parts[0].strip(), 'address': parts[1].strip()})
                else:
                    rows.append({'pattern': 'domain', 'address': item.strip()})
            elif isinstance(item, dict):
                for key, value in item.items():
                    if isinstance(value, list):
                        for v in value:
                            rows.append({'pattern': key, 'address': v})
                    else:
                        rows.append({'pattern': key, 'address': value})
        
        df = pd.DataFrame(rows)
        return df, []  # YAMLé€šå¸¸æ²¡æœ‰é€»è¾‘è§„åˆ™
    
    def _process_text_file(self, text_file: str) -> Tuple[pd.DataFrame, List[Dict]]:
        """å¤„ç†æ–‡æœ¬æ–‡ä»¶"""
        csv_content = self.file_utils.read_text_file(text_file)
        csv_data = StringIO('\n'.join(csv_content))
        df = pd.read_csv(csv_data, header=None, 
                        names=['pattern', 'address', 'other', 'other2', 'other3'], 
                        on_bad_lines='skip')
        
        filtered_rows = []
        rules = []
        
        # å¤„ç†é€»è¾‘è§„åˆ™
        if 'AND' in df['pattern'].values:
            and_rows = df[df['pattern'].str.contains('AND', na=False)]
            for _, row in and_rows.iterrows():
                rule = {
                    "type": "logical",
                    "mode": "and",
                    "rules": []
                }
                pattern = ",".join(row.values.astype(str))
                components = re.findall(r'\((.*?)\)', pattern)
                for component in components:
                    for keyword in self.MAP_DICT.keys():
                        if keyword in component:
                            match = re.search(f'{keyword},(.*)', component)
                            if match:
                                value = match.group(1)
                                rule["rules"].append({
                                    self.MAP_DICT[keyword]: value
                                })
                rules.append(rule)
        
        for index, row in df.iterrows():
            if 'AND' not in row['pattern']:
                filtered_rows.append(row)
        
        df_filtered = pd.DataFrame(filtered_rows, columns=['pattern', 'address', 'other', 'other2', 'other3'])
        return df_filtered, rules
    
    def _merge_rules_to_dict(self, df: pd.DataFrame, merged_by_type: Dict, domain_entries: set) -> None:
        """å°†DataFrameä¸­çš„è§„åˆ™åˆå¹¶åˆ°å­—å…¸ä¸­"""
        for pattern, addresses in df.groupby('pattern')['address'].apply(list).to_dict().items():
            stripped = {addr.strip() for addr in addresses}
            mapped_pattern = self.MAP_DICT.get(pattern, pattern)
            
            if mapped_pattern == 'domain':
                domain_entries.update(stripped)
            else:
                if mapped_pattern not in merged_by_type:
                    merged_by_type[mapped_pattern] = set()
                merged_by_type[mapped_pattern].update(stripped)
    


    def sort_dict(self, data: Dict) -> Dict:
        """
        é€’å½’æ’åºå­—å…¸ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        
        Args:
            data: è¦æ’åºçš„å­—å…¸æˆ–æ•°æ®
            
        Returns:
            æ’åºåçš„å­—å…¸
        """
        if isinstance(data, dict):
            return {k: self.sort_dict(v) for k, v in sorted(data.items())}
        elif isinstance(data, list):
            return [self.sort_dict(item) if isinstance(item, (dict, list)) else item for item in data]
        else:
            return data