"""
è½¬æ¢æœåŠ¡
å¤„ç†converté…ç½®ä¸­çš„é“¾æ¥ï¼Œä½¿ç”¨åŸæœ‰çš„è½¬æ¢é€»è¾‘ç”ŸæˆJSONè§„åˆ™é›†
èå…¥ç°æœ‰çš„ä¸‹è½½-å¤„ç†-ç¼–è¯‘æ¶æ„
"""

import json
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd

from ..utils.config import ConfigManager
from ..utils.file_utils import FileUtils
from ..utils.logger import Logger
from ..utils.network import NetworkUtils
from .downloader import DownloadedData


class ConvertedData:
    """è½¬æ¢æ•°æ®ç»“æœç±»"""

    def __init__(self, convert_name: str):
        self.convert_name = convert_name
        self.json_files: List[str] = []
        self.srs_files: List[str] = []
        self.success_count = 0
        self.total_count = 0
        self.errors: List[str] = []

    def add_converted_file(self, json_file: str, srs_file: str) -> None:
        """æ·»åŠ è½¬æ¢æˆåŠŸçš„æ–‡ä»¶"""
        self.json_files.append(json_file)
        self.srs_files.append(srs_file)
        self.success_count += 1

    def add_error(self, error: str) -> None:
        """æ·»åŠ é”™è¯¯ä¿¡æ¯"""
        self.errors.append(error)

    def set_total_count(self, count: int) -> None:
        """è®¾ç½®æ€»æ•°é‡"""
        self.total_count = count

    def is_successful(self) -> bool:
        """æ˜¯å¦æœ‰æˆåŠŸè½¬æ¢çš„æ•°æ®"""
        return self.success_count > 0


class ConverterService:
    """è½¬æ¢æœåŠ¡ç±»"""

    def __init__(
        self,
        config_manager: ConfigManager,
        logger: Logger,
        network_utils: NetworkUtils,
        file_utils: FileUtils,
    ):
        """
        åˆå§‹åŒ–è½¬æ¢æœåŠ¡

        Args:
            config_manager: é…ç½®ç®¡ç†å™¨
            logger: æ—¥å¿—è®°å½•å™¨
            network_utils: ç½‘ç»œå·¥å…·
            file_utils: æ–‡ä»¶å·¥å…·
        """
        self.config_manager = config_manager
        self.logger = logger
        self.network_utils = network_utils
        self.file_utils = file_utils

        # æ˜ å°„å­—å…¸ - ä»åŸæœ‰convert.pyç§»æ¤
        self.MAP_DICT = {
            "DOMAIN-SUFFIX": "domain_suffix",
            "HOST-SUFFIX": "domain_suffix",
            "host-suffix": "domain_suffix",
            "DOMAIN": "domain",
            "HOST": "domain",
            "host": "domain",
            "DOMAIN-KEYWORD": "domain_keyword",
            "HOST-KEYWORD": "domain_keyword",
            "host-keyword": "domain_keyword",
            "IP-CIDR": "ip_cidr",
            "ip-cidr": "ip_cidr",
            "IP-CIDR6": "ip_cidr",
            "IP6-CIDR": "ip_cidr",
            "SRC-IP-CIDR": "source_ip_cidr",
            "GEOIP": "geoip",
            "DST-PORT": "port",
            "SRC-PORT": "source_port",
            "URL-REGEX": "domain_regex",
            "DOMAIN-REGEX": "domain_regex",
        }

    def convert_downloaded_rulesets(
        self, download_results: Dict[str, DownloadedData]
    ) -> Dict[str, ConvertedData]:
        """
        è½¬æ¢å·²ä¸‹è½½çš„convertè§„åˆ™é›†æ•°æ®

        Args:
            download_results: å·²ä¸‹è½½çš„convertæ•°æ®

        Returns:
            convertè§„åˆ™é›†åç§°åˆ°è½¬æ¢æ•°æ®çš„æ˜ å°„
        """
        if not download_results:
            self.logger.info("ğŸ“‹ æ²¡æœ‰å·²ä¸‹è½½çš„convertæ•°æ®ï¼Œè·³è¿‡è½¬æ¢é˜¶æ®µ")
            return {}

        results = {}

        self.logger.header("å¼€å§‹è½¬æ¢é˜¶æ®µ")
        self.logger.info(f"ğŸ“‹ å¤„ç† {len(download_results)} ä¸ªå·²ä¸‹è½½çš„convertè§„åˆ™é›†")

        for i, (convert_name, download_data) in enumerate(download_results.items(), 1):
            self.logger.step(f"è½¬æ¢è§„åˆ™é›†: {convert_name}", i, len(download_results))

            try:
                converted_data = self._convert_downloaded_data(
                    convert_name, download_data
                )
                results[convert_name] = converted_data

            except Exception as e:
                self.logger.error(f"âŒ è§„åˆ™é›† {convert_name} è½¬æ¢å¼‚å¸¸: {str(e)}")
                # åˆ›å»ºå¤±è´¥çš„è½¬æ¢æ•°æ®
                failed_data = ConvertedData(convert_name)
                failed_data.set_total_count(download_data.total_count)
                failed_data.add_error(f"è½¬æ¢å¼‚å¸¸: {str(e)}")
                results[convert_name] = failed_data

            # æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if i < len(download_results):
                self.logger.info("â”€" * 50)

        # è¾“å‡ºæ€»ä½“ç»Ÿè®¡
        stats = self.get_convert_statistics(results)
        self.logger.separator("convertç»„ è½¬æ¢é˜¶æ®µå®Œæˆ")
        self.logger.success(
            f"âœ… convertç»„ è½¬æ¢å®Œæˆ: {stats['successful_converts']}/{stats['total_converts']} ä¸ªè§„åˆ™é›†æˆåŠŸ"
        )

        return results

    def _convert_downloaded_data(
        self, convert_name: str, download_data: DownloadedData
    ) -> ConvertedData:
        """
        è½¬æ¢å•ä¸ªå·²ä¸‹è½½çš„convertæ•°æ®

        Args:
            convert_name: convertè§„åˆ™é›†åç§°
            download_data: å·²ä¸‹è½½çš„æ•°æ®

        Returns:
            è½¬æ¢æ•°æ®ç»“æœ
        """
        self.logger.info(f"ğŸ”„ è½¬æ¢å·²ä¸‹è½½çš„è§„åˆ™é›†: {convert_name}")

        # åˆ›å»ºè½¬æ¢ç»“æœå¯¹è±¡
        converted_data = ConvertedData(convert_name)
        converted_data.set_total_count(download_data.total_count)

        if not download_data.is_successful():
            converted_data.add_error("æºæ•°æ®ä¸‹è½½å¤±è´¥")
            return converted_data

        # è·å–è¾“å‡ºç›®å½•é…ç½®
        output_config = self.config_manager.get_output_config()
        json_dir = Path(output_config["json_dir"])
        self.file_utils.ensure_dir(json_dir)

        # åˆå§‹åŒ–åˆå¹¶ç»“æ„
        merged_by_type = {}  # pattern -> set of stripped addresses (for dedup)
        all_logic_rules = []  # æ”¶é›†æ‰€æœ‰é€»è¾‘è§„åˆ™
        domain_entries = set()  # å•ç‹¬æ”¶é›†domainï¼Œç”¨äºå»é‡å¹¶æ’å…¥å¼€å¤´

        # å¤„ç†æ–‡æœ¬æ–‡ä»¶
        for text_file in download_data.text_files:
            try:
                self.logger.info(f"ğŸ”„ å¤„ç†æ–‡æœ¬æ–‡ä»¶: {Path(text_file).name}")

                # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è§£æ
                content = self.file_utils.read_text_file(text_file)

                # å°è¯•è§£æä¸ºYAML
                try:
                    import yaml

                    yaml_data = yaml.safe_load("\n".join(content))
                    
                    # æ£€æŸ¥ YAML è§£æç»“æœæ˜¯å¦ä¸ºæœ‰æ•ˆç»“æ„ï¼ˆdict æˆ– listï¼‰
                    # å¦‚æœ yaml.safe_load è¿”å›å­—ç¬¦ä¸²ï¼Œè¯´æ˜æ–‡ä»¶ä¸æ˜¯æ ‡å‡† YAML ç»“æ„
                    # ï¼ˆä¾‹å¦‚ Clash .list æ ¼å¼æ–‡ä»¶ä¼šè¢«è§£æä¸ºå•è¡Œå­—ç¬¦ä¸²ï¼‰
                    if isinstance(yaml_data, (dict, list)):
                        df, logic_rules = self._parse_yaml_data(yaml_data)
                    else:
                        # ä¸æ˜¯æœ‰æ•ˆçš„ YAML ç»“æ„ï¼ŒæŒ‰æ–‡æœ¬åˆ—è¡¨å¤„ç†
                        self.logger.info("ğŸ“ æ£€æµ‹åˆ°é YAML ç»“æ„æ ¼å¼ï¼Œä½¿ç”¨æ–‡æœ¬åˆ—è¡¨è§£æ")
                        df, logic_rules = self._parse_text_list(content)
                except Exception:
                    # å¦‚æœ YAML è§£æå¤±è´¥ï¼ŒæŒ‰æ–‡æœ¬åˆ—è¡¨å¤„ç†
                    df, logic_rules = self._parse_text_list(content)

                # æ”¶é›†é€»è¾‘è§„åˆ™
                all_logic_rules.extend(logic_rules)

                # å¤„ç†DataFrameæ•°æ®
                if df is not None and not df.empty:
                    self._merge_dataframe_to_rules(df, merged_by_type, domain_entries)

                converted_data.success_count += 1

            except Exception as e:
                self.logger.warning(
                    f"âš ï¸ æ–‡ä»¶å¤„ç†å¤±è´¥: {Path(text_file).name} - {str(e)}"
                )
                converted_data.add_error(
                    f"æ–‡ä»¶å¤„ç†å¤±è´¥: {Path(text_file).name} - {str(e)}"
                )

        # å¦‚æœæœ‰æˆåŠŸå¤„ç†çš„æ•°æ®ï¼Œæ„å»ºåˆå¹¶çš„è§„åˆ™é›†
        if merged_by_type or all_logic_rules or domain_entries:
            merged_ruleset = {"version": self.config_manager.get_version(), "rules": []}

            # æ·»åŠ édomainè§„åˆ™
            for pattern, values in merged_by_type.items():
                if values:
                    sorted_values = sorted(list(values))
                    merged_ruleset["rules"].append({pattern: sorted_values})

            # æ·»åŠ domainï¼ˆæ’å…¥å¼€å¤´ï¼Œå»é‡ï¼‰
            if domain_entries:
                sorted_domains = sorted(list(domain_entries))
                merged_ruleset["rules"].insert(0, {"domain": sorted_domains})

            # æ·»åŠ é€»è¾‘è§„åˆ™ï¼ˆè¿½åŠ åˆ°æœ«å°¾ï¼‰
            merged_ruleset["rules"].extend(all_logic_rules)

            # ç”Ÿæˆæ–‡ä»¶å
            file_name = json_dir / f"{convert_name}.json"

            # å†™å…¥JSONæ–‡ä»¶
            with open(file_name, "w", encoding="utf-8") as output_file:
                result_rules_str = json.dumps(
                    self.sort_dict(merged_ruleset), ensure_ascii=False, indent=2
                )
                result_rules_str = result_rules_str.replace("\\\\", "\\")
                output_file.write(result_rules_str)

            converted_data.add_converted_file(str(file_name), "")
            self.logger.info(f"âœ… è½¬æ¢å®Œæˆ: {file_name}")
        else:
            self.logger.error(f"âŒ è§„åˆ™é›† {convert_name} æ— æœ‰æ•ˆæ•°æ®")

        return converted_data

    def _parse_yaml_data(
        self, yaml_data: Any
    ) -> Tuple[Optional[pd.DataFrame], List[Dict]]:
        """
        è§£æYAMLæ•°æ®

        Args:
            yaml_data: YAMLæ•°æ®

        Returns:
            (DataFrameæ•°æ®, é€»è¾‘è§„åˆ™åˆ—è¡¨)
        """
        rows = []
        if not isinstance(yaml_data, str):
            items = (
                yaml_data.get("payload", [])
                if isinstance(yaml_data, dict)
                else yaml_data
            )
        else:
            items = yaml_data.splitlines()

        for item in items:
            if isinstance(item, str):
                # ç®€å•å¤„ç†ï¼Œå‡è®¾æ¯è¡Œæ˜¯ä¸€ä¸ªpattern:addresså¯¹
                parts = item.split(",", 1)
                if len(parts) == 2:
                    rows.append(
                        {"pattern": parts[0].strip(), "address": parts[1].strip()}
                    )
                else:
                    rows.append({"pattern": "domain", "address": item.strip()})
            elif isinstance(item, dict):
                for key, value in item.items():
                    if isinstance(value, list):
                        for v in value:
                            rows.append({"pattern": key, "address": v})
                    else:
                        rows.append({"pattern": key, "address": value})

        df = pd.DataFrame(rows) if rows else None
        return df, []  # YAMLé€šå¸¸æ²¡æœ‰é€»è¾‘è§„åˆ™

    def _parse_text_list(
        self, content: List[str]
    ) -> Tuple[Optional[pd.DataFrame], List[Dict]]:
        """
        è§£ææ–‡æœ¬åˆ—è¡¨æ•°æ®

        Args:
            content: æ–‡æœ¬å†…å®¹è¡Œåˆ—è¡¨

        Returns:
            (DataFrameæ•°æ®, é€»è¾‘è§„åˆ™åˆ—è¡¨)
        """
        from io import StringIO

        # è¿‡æ»¤æ‰æ³¨é‡Šè¡Œï¼ˆä»¥ # å¼€å¤´ï¼‰å’Œç©ºè¡Œ
        filtered_content = [
            line for line in content 
            if line.strip() and not line.strip().startswith("#")
        ]

        csv_data = StringIO("\n".join(filtered_content))
        df = pd.read_csv(
            csv_data,
            header=None,
            names=["pattern", "address", "other", "other2", "other3"],
            on_bad_lines="skip",
        )

        filtered_rows = []
        rules = []

        # å¤„ç†é€»è¾‘è§„åˆ™
        if "AND" in df["pattern"].values:
            and_rows = df[df["pattern"].str.contains("AND", na=False)]
            for _, row in and_rows.iterrows():
                rule = {"type": "logical", "mode": "and", "rules": []}
                pattern = ",".join(row.values.astype(str))
                components = re.findall(r"\((.*?)\)", pattern)
                for component in components:
                    for keyword in self.MAP_DICT.keys():
                        if keyword in component:
                            match = re.search(f"{keyword},(.*)", component)
                            if match:
                                value = match.group(1)
                                rule["rules"].append({self.MAP_DICT[keyword]: value})
                rules.append(rule)

        for index, row in df.iterrows():
            if "AND" not in row["pattern"]:
                filtered_rows.append(row)

        df_filtered = pd.DataFrame(
            filtered_rows, columns=["pattern", "address", "other", "other2", "other3"]
        )
        return df_filtered, rules

    def _merge_dataframe_to_rules(
        self, df: pd.DataFrame, merged_by_type: Dict, domain_entries: set
    ) -> None:
        """
        å°†DataFrameæ•°æ®åˆå¹¶åˆ°è§„åˆ™é›†ä¸­

        Args:
            df: è¦å¤„ç†çš„DataFrame
            merged_by_type: æŒ‰ç±»å‹åˆ†ç»„çš„è§„åˆ™å­—å…¸
            domain_entries: domainæ¡ç›®é›†åˆ
        """
        # è¿‡æ»¤æ‰åŒ…å«ANDçš„è¡Œ
        filtered_rows = []
        for index, row in df.iterrows():
            if "AND" not in str(row.get("pattern", "")):
                filtered_rows.append(row)

        if not filtered_rows:
            return

        df_filtered = pd.DataFrame(filtered_rows)

        # æŒ‰patternåˆ†ç»„å¹¶åˆå¹¶
        for pattern, addresses in (
            df_filtered.groupby("pattern")["address"].apply(list).to_dict().items()
        ):
            # æ£€æŸ¥ pattern æ˜¯å¦åœ¨ MAP_DICT ä¸­ï¼Œä¸åœ¨åˆ™è·³è¿‡
            if pattern not in self.MAP_DICT:
                self.logger.info(f"â­ï¸ è·³è¿‡ä¸æ”¯æŒçš„è§„åˆ™ç±»å‹: {pattern}")
                continue
            
            stripped = {str(addr).strip() for addr in addresses}  # set for dedup
            mapped_pattern = self.MAP_DICT[pattern]  # æ˜ å°„åˆ°æ ‡å‡†ç±»å‹

            if mapped_pattern == "domain":
                domain_entries.update(stripped)
            else:
                if mapped_pattern not in merged_by_type:
                    merged_by_type[mapped_pattern] = set()
                merged_by_type[mapped_pattern].update(stripped)

    def get_convert_statistics(
        self, results: Dict[str, ConvertedData]
    ) -> Dict[str, Any]:
        """
        è·å–è½¬æ¢ç»Ÿè®¡ä¿¡æ¯

        Args:
            results: è½¬æ¢ç»“æœå­—å…¸

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total_converts = len(results)
        successful_converts = sum(
            1 for data in results.values() if data.is_successful()
        )
        total_urls = sum(data.total_count for data in results.values())
        successful_urls = sum(data.success_count for data in results.values())
        total_json_files = sum(len(data.json_files) for data in results.values())

        return {
            "total_converts": total_converts,
            "successful_converts": successful_converts,
            "total_urls": total_urls,
            "successful_urls": successful_urls,
            "total_json_files": total_json_files,
            "success_rate": (
                (successful_urls / total_urls * 100) if total_urls > 0 else 0
            ),
        }

    def sort_dict(self, data: Dict) -> Dict:
        """
        é€’å½’æ’åºå­—å…¸ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰

        Args:
            data: è¦æ’åºçš„å­—å…¸æˆ–æ•°æ®

        Returns:
            æ’åºåçš„å­—å…¸
        """
        if isinstance(data, dict):
            return {k: self.sort_dict(v) for k, v in sorted(data.items())}
        elif isinstance(data, list):
            return [
                self.sort_dict(item) if isinstance(item, (dict, list)) else item
                for item in data
            ]
        else:
            return data
