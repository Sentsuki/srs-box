"""
å¤„ç†æœåŠ¡
å®ç°JSONè§„åˆ™é›†åˆå¹¶ã€IPåˆ—è¡¨å¤„ç†ã€è§„åˆ™è¿‡æ»¤åŠŸèƒ½
ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œæ”¯æŒå¤§æ–‡ä»¶å¤„ç†
"""

import json
from pathlib import Path
from typing import Any, Dict, Generator, List, Optional, Set, Tuple

from ..utils.config import ConfigManager
from ..utils.file_utils import FileUtils
from ..utils.logger import Logger
from .downloader import DownloadedData


class ProcessedData:
    """å¤„ç†åçš„æ•°æ®ç»“æœç±»"""

    def __init__(self, ruleset_name: str):
        self.ruleset_name = ruleset_name
        self.ruleset_data: Optional[Dict[str, Any]] = None
        self.output_file: Optional[str] = None
        self.rule_count = 0
        self.rule_types: List[str] = []
        self.filtered_count = 0
        self.success = False
        self.error: Optional[str] = None

    def set_success(
        self,
        ruleset_data: Dict[str, Any],
        output_file: str,
        rule_count: int,
        rule_types: List[str],
        filtered_count: int = 0,
    ) -> None:
        """è®¾ç½®æˆåŠŸç»“æœ"""
        self.ruleset_data = ruleset_data
        self.output_file = output_file
        self.rule_count = rule_count
        self.rule_types = rule_types
        self.filtered_count = filtered_count
        self.success = True

    def set_error(self, error: str) -> None:
        """è®¾ç½®é”™è¯¯ç»“æœ"""
        self.error = error
        self.success = False


class ProcessorService:
    """å¤„ç†æœåŠ¡ç±»"""

    def __init__(
        self, config_manager: ConfigManager, logger: Logger, file_utils: FileUtils
    ):
        """
        åˆå§‹åŒ–å¤„ç†æœåŠ¡

        Args:
            config_manager: é…ç½®ç®¡ç†å™¨
            logger: æ—¥å¿—è®°å½•å™¨
            file_utils: æ–‡ä»¶å·¥å…·
        """
        self.config_manager = config_manager
        self.logger = logger
        self.file_utils = file_utils

        # è¿‡æ»¤å…³é”®è¯é…ç½®
        self.filter_keywords = ["ruleset.skk.moe"]

    def should_filter_rule_value(self, value: str) -> bool:
        """
        æ£€æŸ¥è§„åˆ™å€¼æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤æ‰

        Args:
            value: è§„åˆ™å€¼

        Returns:
            æ˜¯å¦åº”è¯¥è¿‡æ»¤
        """
        if not isinstance(value, str):
            return False

        value_lower = value.lower()
        return any(keyword in value_lower for keyword in self.filter_keywords)

    def filter_rules(
        self, rules: List[Dict[str, Any]]
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        è¿‡æ»¤è§„åˆ™åˆ—è¡¨ï¼Œç§»é™¤åŒ…å«ç‰¹å®šå…³é”®å­—çš„è§„åˆ™ï¼Œä½¿ç”¨å†…å­˜ä¼˜åŒ–çš„æµå¼å¤„ç†

        Args:
            rules: è§„åˆ™åˆ—è¡¨

        Returns:
            (è¿‡æ»¤åçš„è§„åˆ™åˆ—è¡¨, è¢«è¿‡æ»¤çš„è§„åˆ™æ•°é‡)
        """
        filtered_rules = []
        filtered_count = 0

        # åˆ†æ‰¹å¤„ç†è§„åˆ™ï¼Œé¿å…å†…å­˜å³°å€¼
        batch_size = 100

        for batch_start in range(0, len(rules), batch_size):
            batch_end = min(batch_start + batch_size, len(rules))
            rule_batch = rules[batch_start:batch_end]

            for rule in rule_batch:
                if not isinstance(rule, dict):
                    continue

                filtered_rule = {}

                for rule_type, rule_values in rule.items():
                    if not isinstance(rule_values, list):
                        continue

                    # åˆ†æ‰¹è¿‡æ»¤è§„åˆ™å€¼ï¼Œé¿å…å¤§é‡è§„åˆ™å€¼åŒæ—¶åœ¨å†…å­˜ä¸­
                    original_count = len(rule_values)
                    filtered_values = []

                    value_batch_size = 1000
                    for value_start in range(0, len(rule_values), value_batch_size):
                        value_end = min(
                            value_start + value_batch_size, len(rule_values)
                        )
                        value_batch = rule_values[value_start:value_end]

                        # è¿‡æ»¤å½“å‰æ‰¹æ¬¡çš„å€¼
                        batch_filtered = [
                            value
                            for value in value_batch
                            if not self.should_filter_rule_value(value)
                        ]
                        filtered_values.extend(batch_filtered)

                        # æ¸…ç†å·²å¤„ç†çš„æ‰¹æ¬¡
                        del value_batch
                        del batch_filtered

                    filtered_count += original_count - len(filtered_values)

                    # åªæ·»åŠ éç©ºçš„è§„åˆ™
                    if filtered_values:
                        filtered_rule[rule_type] = filtered_values

                # åªæ·»åŠ éç©ºçš„è§„åˆ™å¯¹è±¡
                if filtered_rule:
                    filtered_rules.append(filtered_rule)

            # æ˜¾ç¤ºå¤„ç†è¿›åº¦ï¼ˆå¯¹äºå¤§è§„åˆ™é›†ï¼‰
            if batch_end % 1000 == 0:
                self.logger.info(f"ğŸ”„ è¿‡æ»¤è¿›åº¦: {batch_end}/{len(rules)} è§„åˆ™")

        return filtered_rules, filtered_count

    def cleanup_temporary_data(self) -> None:
        """
        æ¸…ç†ä¸´æ—¶æ•°æ®å’Œæ–‡ä»¶ï¼Œé‡Šæ”¾å†…å­˜
        """
        try:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_dir = Path("temp")
            if temp_dir.exists():
                # æ¸…ç†å¤„ç†è¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ–‡ä»¶
                for temp_file in temp_dir.glob("*.tmp"):
                    try:
                        temp_file.unlink()
                    except OSError:
                        pass

                # æ¸…ç†ç©ºçš„å­ç›®å½•
                for subdir in temp_dir.iterdir():
                    if subdir.is_dir():
                        try:
                            # å¦‚æœç›®å½•ä¸ºç©ºï¼Œåˆ é™¤å®ƒ
                            subdir.rmdir()
                        except OSError:
                            pass  # ç›®å½•ä¸ä¸ºç©ºæˆ–æ— æ³•åˆ é™¤

            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc

            gc.collect()

            self.logger.info("ğŸ§¹ ä¸´æ—¶æ•°æ®æ¸…ç†å®Œæˆ")

        except Exception as e:
            self.logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ•°æ®æ—¶å‡ºé”™: {str(e)}")

    def get_memory_usage_info(self) -> Dict[str, Any]:
        """
        è·å–å†…å­˜ä½¿ç”¨ä¿¡æ¯

        Returns:
            å†…å­˜ä½¿ç”¨ä¿¡æ¯å­—å…¸
        """
        try:
            import os

            import psutil

            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()

            return {
                "rss_mb": memory_info.rss / (1024 * 1024),  # ç‰©ç†å†…å­˜ä½¿ç”¨
                "vms_mb": memory_info.vms / (1024 * 1024),  # è™šæ‹Ÿå†…å­˜ä½¿ç”¨
                "percent": process.memory_percent(),  # å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
                "available_mb": psutil.virtual_memory().available / (1024 * 1024),
            }
        except ImportError:
            # å¦‚æœæ²¡æœ‰psutilï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            return {
                "rss_mb": 0,
                "vms_mb": 0,
                "percent": 0,
                "available_mb": 0,
                "note": "psutil not available",
            }

    def merge_json_rulesets(
        self, json_data_list: List[Dict[str, Any]], config_version: int
    ) -> Dict[str, Any]:
        """
        æ™ºèƒ½åˆå¹¶å¤šä¸ªJSONè§„åˆ™é›†ï¼Œä½¿ç”¨å†…å­˜ä¼˜åŒ–çš„æµå¼å¤„ç†

        Args:
            json_data_list: JSONæ•°æ®åˆ—è¡¨
            config_version: é…ç½®ç‰ˆæœ¬å·

        Returns:
            åˆå¹¶åçš„è§„åˆ™é›†
        """
        # ç”¨äºå­˜å‚¨åˆå¹¶åçš„è§„åˆ™ï¼ŒæŒ‰è§„åˆ™ç±»å‹åˆ†ç»„
        rule_groups: Dict[str, Set[str]] = {}

        # æµå¼å¤„ç†æ¯ä¸ªJSONæ•°æ®ï¼Œé¿å…åŒæ—¶åœ¨å†…å­˜ä¸­ä¿å­˜æ‰€æœ‰æ•°æ®
        # åˆ›å»ºç´¢å¼•åˆ—è¡¨ï¼Œé¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹åŸåˆ—è¡¨
        for i, json_data in enumerate(json_data_list, 1):
            if json_data is None:  # è·³è¿‡å·²æ¸…ç†çš„æ•°æ®
                continue

            self.logger.info(f"ğŸ”„ åˆå¹¶JSONè§„åˆ™é›† {i}/{len(json_data_list)}")

            try:
                rules = []

                # æå–è§„åˆ™åˆ—è¡¨
                if "rules" in json_data and isinstance(json_data["rules"], list):
                    rules = json_data["rules"]
                else:
                    # å¦‚æœJSONç»“æ„ä¸æ ‡å‡†ï¼Œå°è¯•ç›´æ¥ä½œä¸ºè§„åˆ™å¤„ç†
                    rules = [json_data]

                # æµå¼å¤„ç†æ¯ä¸ªè§„åˆ™ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰è§„åˆ™åˆ°å†…å­˜
                for rule_index, rule in enumerate(rules):
                    if not isinstance(rule, dict):
                        continue

                    # éå†è§„åˆ™ä¸­çš„æ¯ä¸ªå­—æ®µ
                    for rule_type, rule_values in rule.items():
                        if not isinstance(rule_values, list):
                            continue

                        # å¦‚æœè¿™ä¸ªè§„åˆ™ç±»å‹è¿˜æ²¡æœ‰ï¼Œåˆ›å»ºæ–°çš„é›†åˆ
                        if rule_type not in rule_groups:
                            rule_groups[rule_type] = set()

                        # åˆ†æ‰¹å¤„ç†è§„åˆ™å€¼ï¼Œé¿å…å†…å­˜å³°å€¼
                        batch_size = 1000
                        for batch_start in range(0, len(rule_values), batch_size):
                            batch_end = min(batch_start + batch_size, len(rule_values))
                            batch_values = rule_values[batch_start:batch_end]

                            # åˆå¹¶è§„åˆ™å€¼ï¼Œè‡ªåŠ¨å»é‡
                            for value in batch_values:
                                if isinstance(value, str):
                                    rule_groups[rule_type].add(value)

                    # æ˜¾ç¤ºå¤„ç†è¿›åº¦ï¼ˆå¯¹äºå¤§è§„åˆ™é›†ï¼‰
                    if rule_index > 0 and rule_index % 100 == 0:
                        self.logger.info(f"   å¤„ç†è§„åˆ™: {rule_index + 1}/{len(rules)}")

            except Exception as e:
                self.logger.warning(f"âš ï¸ å¤„ç†JSONè§„åˆ™é›† {i} æ—¶å‡ºé”™: {str(e)}")
                continue

        # åœ¨å¤„ç†å®Œæ‰€æœ‰æ•°æ®åæ¸…ç†åˆ—è¡¨
        json_data_list.clear()

        # å°†åˆ†ç»„çš„è§„åˆ™è½¬æ¢ä¸ºæœ€ç»ˆæ ¼å¼ï¼Œä½¿ç”¨å†…å­˜ä¼˜åŒ–çš„æ–¹å¼
        merged_rules = []

        # åˆ›å»ºè§„åˆ™ç±»å‹åˆ—è¡¨çš„å‰¯æœ¬ï¼Œé¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹å­—å…¸
        rule_types_to_process = list(rule_groups.keys())

        for rule_type in rule_types_to_process:
            rule_values = rule_groups[rule_type]
            if rule_values:  # åªæ·»åŠ éç©ºçš„è§„åˆ™
                self.logger.info(
                    f"ğŸ”„ æ’åºè§„åˆ™ç±»å‹ {rule_type}: {len(rule_values)} æ¡è§„åˆ™"
                )

                # å¯¹äºå¤§é‡è§„åˆ™ï¼Œä½¿ç”¨åˆ†å—æ’åº
                if len(rule_values) > 10000:
                    # åˆ†å—å¤„ç†å¤§é‡æ•°æ®
                    sorted_values = []
                    chunk_size = 5000
                    value_list = list(rule_values)

                    for chunk_start in range(0, len(value_list), chunk_size):
                        chunk_end = min(chunk_start + chunk_size, len(value_list))
                        chunk = value_list[chunk_start:chunk_end]
                        sorted_chunk = sorted(chunk)
                        sorted_values.extend(sorted_chunk)

                        # æ¸…ç†å·²å¤„ç†çš„å—
                        del chunk
                        del sorted_chunk

                    # æ¸…ç†ä¸´æ—¶åˆ—è¡¨
                    del value_list
                else:
                    # å°é‡æ•°æ®ç›´æ¥æ’åº
                    sorted_values = sorted(list(rule_values))

                merged_rules.append({rule_type: sorted_values})

                # æ¸…ç†å·²å¤„ç†çš„è§„åˆ™ç»„ï¼Œé‡Šæ”¾å†…å­˜
                del rule_groups[rule_type]

        # åˆ›å»ºåˆå¹¶åçš„è§„åˆ™é›†
        merged_ruleset = {"version": config_version, "rules": merged_rules}

        self.logger.info(f"âœ… JSONè§„åˆ™é›†åˆå¹¶å®Œæˆï¼Œå…± {len(merged_rules)} ç§è§„åˆ™ç±»å‹")

        return merged_ruleset

    def create_ip_ruleset_from_text_files(
        self, text_files: List[str], config_version: int
    ) -> Dict[str, Any]:
        """
        ä»æ–‡æœ¬æ–‡ä»¶åˆ›å»ºIPè§„åˆ™é›†ï¼Œä½¿ç”¨æµå¼å¤„ç†ä¼˜åŒ–å†…å­˜ä½¿ç”¨

        Args:
            text_files: æ–‡æœ¬æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            config_version: é…ç½®ç‰ˆæœ¬å·

        Returns:
            IPè§„åˆ™é›†æ•°æ®
        """

        # ä½¿ç”¨ç”Ÿæˆå™¨æµå¼å¤„ç†å¤§æ–‡ä»¶ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
        def read_ip_lines_streaming() -> Generator[str, None, None]:
            """æµå¼è¯»å–IPè¡Œï¼Œé€è¡Œå¤„ç†é¿å…åŠ è½½æ•´ä¸ªæ–‡ä»¶åˆ°å†…å­˜"""
            for file_path in text_files:
                try:
                    # ä½¿ç”¨æµå¼è¯»å–ï¼Œä¸€æ¬¡åªè¯»å–ä¸€è¡Œ
                    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                        for line_num, line in enumerate(f, 1):
                            cleaned_line = line.strip()
                            if cleaned_line and not cleaned_line.startswith("#"):
                                yield cleaned_line

                            # æ¯å¤„ç†1000è¡Œæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ï¼ˆå¯¹äºå¤§æ–‡ä»¶ï¼‰
                            if line_num % 1000 == 0:
                                self.logger.info(
                                    f"ğŸ“– å¤„ç†æ–‡ä»¶ {Path(file_path).name}: {line_num} è¡Œ"
                                )

                except Exception as e:
                    self.logger.warning(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥: {file_path} - {str(e)}")

        # ä½¿ç”¨é›†åˆè¿›è¡Œå†…å­˜é«˜æ•ˆçš„å»é‡ï¼Œåˆ†æ‰¹å¤„ç†é¿å…å†…å­˜å³°å€¼
        ip_set = set()
        batch_size = 10000  # æ¯æ‰¹å¤„ç†10000ä¸ªIP
        batch_count = 0

        current_batch = []
        for ip in read_ip_lines_streaming():
            current_batch.append(ip)

            # å½“æ‰¹æ¬¡è¾¾åˆ°æŒ‡å®šå¤§å°æ—¶ï¼Œå¤„ç†è¿™ä¸€æ‰¹
            if len(current_batch) >= batch_size:
                ip_set.update(current_batch)
                current_batch.clear()  # æ¸…ç©ºå½“å‰æ‰¹æ¬¡ï¼Œé‡Šæ”¾å†…å­˜
                batch_count += 1

                # æ˜¾ç¤ºå¤„ç†è¿›åº¦
                self.logger.info(f"ğŸ”„ å·²å¤„ç† {batch_count * batch_size} ä¸ªIPåœ°å€")

        # å¤„ç†æœ€åä¸€æ‰¹
        if current_batch:
            ip_set.update(current_batch)
            current_batch.clear()

        # è½¬æ¢ä¸ºæ’åºåˆ—è¡¨ï¼Œåˆ†æ‰¹æ’åºä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        self.logger.info(f"ğŸ“Š å»é‡åå…±æœ‰ {len(ip_set)} ä¸ªå”¯ä¸€IPåœ°å€")
        self.logger.info("ğŸ”„ å¼€å§‹æ’åºIPåœ°å€...")

        # å¯¹äºå¤§é‡IPï¼Œä½¿ç”¨åˆ†å—æ’åº
        if len(ip_set) > 50000:
            # åˆ†å—å¤„ç†å¤§é‡æ•°æ®
            ip_list = []
            chunk_size = 10000
            ip_chunks = [
                list(ip_set)[i : i + chunk_size]
                for i in range(0, len(ip_set), chunk_size)
            ]

            for i, chunk in enumerate(ip_chunks, 1):
                sorted_chunk = sorted(chunk)
                ip_list.extend(sorted_chunk)
                self.logger.info(f"ğŸ”„ æ’åºè¿›åº¦: {i}/{len(ip_chunks)} å—")

                # æ¸…ç†å·²å¤„ç†çš„å—ï¼Œé‡Šæ”¾å†…å­˜
                del chunk
                del sorted_chunk
        else:
            # å°é‡æ•°æ®ç›´æ¥æ’åº
            ip_list = sorted(list(ip_set))

        # æ¸…ç†é›†åˆï¼Œé‡Šæ”¾å†…å­˜
        del ip_set

        # åˆ›å»ºè§„åˆ™é›†
        ruleset = {"version": config_version, "rules": [{"ip_cidr": ip_list}]}

        self.logger.info(f"âœ… IPè§„åˆ™é›†åˆ›å»ºå®Œæˆï¼Œå…± {len(ip_list)} æ¡è§„åˆ™")

        return ruleset

    def process_ruleset(
        self, ruleset_name: str, downloaded_data: DownloadedData
    ) -> ProcessedData:
        """
        å¤„ç†å•ä¸ªè§„åˆ™é›†çš„ä¸‹è½½æ•°æ®ï¼Œä½¿ç”¨å†…å­˜ä¼˜åŒ–çš„å¤„ç†æ–¹å¼

        Args:
            ruleset_name: è§„åˆ™é›†åç§°
            downloaded_data: ä¸‹è½½çš„æ•°æ®

        Returns:
            å¤„ç†ç»“æœ
        """
        self.logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†è§„åˆ™é›†: {ruleset_name}")

        # æ˜¾ç¤ºå¤„ç†å‰çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_info = self.get_memory_usage_info()
        if memory_info["rss_mb"] > 0:
            self.logger.info(f"ğŸ’¾ å¤„ç†å‰å†…å­˜ä½¿ç”¨: {memory_info['rss_mb']:.1f} MB")

        processed_data = ProcessedData(ruleset_name)

        try:
            config_version = self.config_manager.get_version()

            # ä¼˜å…ˆå¤„ç†JSONæ•°æ®
            if downloaded_data.has_json_data():
                self.logger.info(
                    f"ğŸ“„ å¤„ç†JSONè§„åˆ™é›†æ•°æ®: {len(downloaded_data.json_data)} ä¸ª"
                )

                if len(downloaded_data.json_data) == 1:
                    # åªæœ‰ä¸€ä¸ªJSONæ–‡ä»¶ï¼Œä½¿ç”¨å¹¶è¦†ç›–ç‰ˆæœ¬å·
                    ruleset_data = downloaded_data.json_data[0]
                    # ç¡®ä¿ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„ç‰ˆæœ¬å·
                    ruleset_data["version"] = config_version
                    self.logger.info("ğŸ“‹ ä½¿ç”¨å•ä¸ªJSONè§„åˆ™é›†å¹¶è¦†ç›–ç‰ˆæœ¬å·")
                else:
                    # å¤šä¸ªJSONæ–‡ä»¶ï¼Œéœ€è¦åˆå¹¶
                    self.logger.info(
                        f"ğŸ”€ åˆå¹¶ {len(downloaded_data.json_data)} ä¸ªJSONè§„åˆ™é›†"
                    )
                    ruleset_data = self.merge_json_rulesets(
                        downloaded_data.json_data, config_version
                    )

                # æ¸…ç†åŸå§‹JSONæ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
                downloaded_data.json_data.clear()

                # è¿‡æ»¤è§„åˆ™
                if "rules" in ruleset_data and isinstance(ruleset_data["rules"], list):
                    self.logger.info("ğŸ”„ å¼€å§‹è¿‡æ»¤è§„åˆ™...")
                    original_rules = ruleset_data["rules"]
                    filtered_rules, filtered_count = self.filter_rules(original_rules)

                    # æ›¿æ¢è§„åˆ™å¹¶æ¸…ç†åŸå§‹æ•°æ®
                    ruleset_data["rules"] = filtered_rules
                    del original_rules  # æ˜¾å¼åˆ é™¤åŸå§‹è§„åˆ™ï¼Œé‡Šæ”¾å†…å­˜

                    if filtered_count > 0:
                        self.logger.info(
                            f"ğŸš« å·²è¿‡æ»¤ {filtered_count} æ¡åŒ…å«è¿‡æ»¤å…³é”®å­—çš„è§„åˆ™"
                        )
                else:
                    filtered_count = 0

                # ç»Ÿè®¡è§„åˆ™ä¿¡æ¯
                rule_count = 0
                rule_types = []

                for rule in ruleset_data.get("rules", []):
                    for rule_type, rule_values in rule.items():
                        if isinstance(rule_values, list):
                            rule_types.append(f"{rule_type}({len(rule_values)})")
                            rule_count += len(rule_values)

                self.logger.info(f"âœ… JSONè§„åˆ™é›†å¤„ç†å®Œæˆ")
                self.logger.info(
                    f"ğŸ“Š è§„åˆ™ç»Ÿè®¡: {', '.join(rule_types)}ï¼Œæ€»è®¡ {rule_count} æ¡è§„åˆ™"
                )

            elif downloaded_data.has_text_files():
                # å¤„ç†æ–‡æœ¬æ–‡ä»¶ï¼ˆIPåˆ—è¡¨ï¼‰
                self.logger.info(
                    f"ğŸ“„ å¤„ç†æ–‡æœ¬æ–‡ä»¶: {len(downloaded_data.text_files)} ä¸ª"
                )

                ruleset_data = self.create_ip_ruleset_from_text_files(
                    downloaded_data.text_files, config_version
                )

                # æ¸…ç†æ–‡æœ¬æ–‡ä»¶åˆ—è¡¨ï¼Œé‡Šæ”¾å†…å­˜
                downloaded_data.text_files.clear()

                # ç»Ÿè®¡IPæ•°é‡
                rule_count = 0
                rule_types = []
                filtered_count = 0

                for rule in ruleset_data.get("rules", []):
                    for rule_type, rule_values in rule.items():
                        if isinstance(rule_values, list):
                            rule_types.append(f"{rule_type}({len(rule_values)})")
                            rule_count += len(rule_values)

                self.logger.info(f"âœ… æ–‡æœ¬è§„åˆ™é›†å¤„ç†å®Œæˆ")
                self.logger.info(
                    f"ğŸ“Š è§„åˆ™ç»Ÿè®¡: {', '.join(rule_types)}ï¼Œæ€»è®¡ {rule_count} æ¡è§„åˆ™"
                )

            else:
                # æ²¡æœ‰å¯å¤„ç†çš„æ•°æ®
                processed_data.set_error("æ²¡æœ‰å¯å¤„ç†çš„ä¸‹è½½æ•°æ®")
                return processed_data

            # æ˜¾ç¤ºå¤„ç†åçš„å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory_info_after = self.get_memory_usage_info()
            if memory_info_after["rss_mb"] > 0:
                self.logger.info(
                    f"ğŸ’¾ å¤„ç†åå†…å­˜ä½¿ç”¨: {memory_info_after['rss_mb']:.1f} MB"
                )

            # è·å–è¾“å‡ºç›®å½•é…ç½®
            output_config = self.config_manager.get_output_config()
            json_dir = output_config["json_dir"]

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            from pathlib import Path

            json_path = Path(json_dir)
            json_path.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜å¤„ç†åçš„è§„åˆ™é›†
            output_file = json_path / f"{ruleset_name}.json"
            self.file_utils.write_json_file(str(output_file), ruleset_data)

            processed_data.set_success(
                ruleset_data, str(output_file), rule_count, rule_types, filtered_count
            )

            self.logger.info(f"âœ… è§„åˆ™é›†å·²ä¿å­˜åˆ°: {output_file}")

            # æ¸…ç†ä¸´æ—¶æ•°æ®
            self.cleanup_temporary_data()

        except Exception as e:
            error_msg = f"å¤„ç†è§„åˆ™é›†æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
            self.logger.error(f"âŒ {error_msg}")
            processed_data.set_error(error_msg)

            # å³ä½¿å‡ºé”™ä¹Ÿè¦æ¸…ç†ä¸´æ—¶æ•°æ®
            self.cleanup_temporary_data()

        return processed_data

    def process_all_rulesets(
        self, download_results: Dict[str, DownloadedData]
    ) -> Dict[str, ProcessedData]:
        """
        å¤„ç†æ‰€æœ‰è§„åˆ™é›†

        Args:
            download_results: ä¸‹è½½ç»“æœå­—å…¸

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        results = {}

        self.logger.header("å¼€å§‹å¤„ç†é˜¶æ®µ")

        # åªå¤„ç†æˆåŠŸä¸‹è½½çš„è§„åˆ™é›†
        successful_downloads = {
            name: data
            for name, data in download_results.items()
            if data.is_successful()
        }

        if not successful_downloads:
            self.logger.warning("âš ï¸ æ²¡æœ‰æˆåŠŸä¸‹è½½çš„è§„åˆ™é›†éœ€è¦å¤„ç†")
            return results

        self.logger.info(f"ğŸ“‹ éœ€è¦å¤„ç† {len(successful_downloads)} ä¸ªè§„åˆ™é›†")

        for i, (ruleset_name, downloaded_data) in enumerate(
            successful_downloads.items(), 1
        ):
            self.logger.step(
                f"å¤„ç†è§„åˆ™é›†: {ruleset_name}", i, len(successful_downloads)
            )

            try:
                processed_data = self.process_ruleset(ruleset_name, downloaded_data)
                results[ruleset_name] = processed_data

            except Exception as e:
                self.logger.error(f"âŒ è§„åˆ™é›† {ruleset_name} å¤„ç†å¼‚å¸¸: {str(e)}")
                # åˆ›å»ºå¤±è´¥çš„å¤„ç†æ•°æ®
                failed_data = ProcessedData(ruleset_name)
                failed_data.set_error(f"å¤„ç†å¼‚å¸¸: {str(e)}")
                results[ruleset_name] = failed_data

            # æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if i < len(successful_downloads):
                self.logger.info("â”€" * 50)

        # è¾“å‡ºæ€»ä½“ç»Ÿè®¡
        successful_processed = sum(1 for data in results.values() if data.success)
        self.logger.separator("å¤„ç†é˜¶æ®µå®Œæˆ")
        self.logger.success(
            f"âœ… å¤„ç†å®Œæˆ: {successful_processed}/{len(successful_downloads)} ä¸ªè§„åˆ™é›†æˆåŠŸ"
        )

        return results

    def get_processing_statistics(
        self, results: Dict[str, ProcessedData]
    ) -> Dict[str, Any]:
        """
        è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯

        Args:
            results: å¤„ç†ç»“æœå­—å…¸

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total_rulesets = len(results)
        successful_rulesets = sum(1 for data in results.values() if data.success)
        total_rules = sum(data.rule_count for data in results.values() if data.success)
        total_filtered = sum(
            data.filtered_count for data in results.values() if data.success
        )

        # ç»Ÿè®¡è§„åˆ™ç±»å‹
        rule_type_counts = {}
        for data in results.values():
            if data.success:
                for rule_type in data.rule_types:
                    # æå–è§„åˆ™ç±»å‹åç§°ï¼ˆå»æ‰æ•°é‡ï¼‰
                    type_name = rule_type.split("(")[0]
                    if type_name not in rule_type_counts:
                        rule_type_counts[type_name] = 0
                    rule_type_counts[type_name] += 1

        return {
            "total_rulesets": total_rulesets,
            "successful_rulesets": successful_rulesets,
            "total_rules": total_rules,
            "total_filtered": total_filtered,
            "rule_type_counts": rule_type_counts,
            "success_rate": (
                (successful_rulesets / total_rulesets * 100)
                if total_rulesets > 0
                else 0
            ),
        }
