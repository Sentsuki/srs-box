"""
ä¸‹è½½æœåŠ¡
å¤„ç†ä¸åŒç±»å‹æ•°æ®æºï¼ˆJSONè§„åˆ™é›†ã€æ–‡æœ¬IPåˆ—è¡¨ï¼‰çš„ä¸‹è½½
é›†æˆç½‘ç»œå·¥å…·å’Œæ–‡ä»¶å·¥å…·ï¼Œæ¶ˆé™¤é‡å¤ä»£ç 
"""

import os
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

from ..utils.config import ConfigManager
from ..utils.logger import Logger
from ..utils.network import NetworkUtils, DownloadResult
from ..utils.file_utils import FileUtils


class DownloadedData:
    """ä¸‹è½½æ•°æ®ç»“æœç±»"""
    
    def __init__(self, ruleset_name: str):
        self.ruleset_name = ruleset_name
        self.json_data: List[Dict[str, Any]] = []
        self.text_files: List[str] = []
        self.success_count = 0
        self.total_count = 0
        self.errors: List[str] = []
    
    def add_json_data(self, data: Dict[str, Any]) -> None:
        """æ·»åŠ JSONæ•°æ®"""
        self.json_data.append(data)
        self.success_count += 1
    
    def add_text_file(self, file_path: str) -> None:
        """æ·»åŠ æ–‡æœ¬æ–‡ä»¶è·¯å¾„"""
        self.text_files.append(file_path)
        self.success_count += 1
    
    def add_error(self, error: str) -> None:
        """æ·»åŠ é”™è¯¯ä¿¡æ¯"""
        self.errors.append(error)
    
    def set_total_count(self, count: int) -> None:
        """è®¾ç½®æ€»æ•°é‡"""
        self.total_count = count
    
    def is_successful(self) -> bool:
        """æ˜¯å¦æœ‰æˆåŠŸä¸‹è½½çš„æ•°æ®"""
        return self.success_count > 0
    
    def has_json_data(self) -> bool:
        """æ˜¯å¦æœ‰JSONæ•°æ®"""
        return len(self.json_data) > 0
    
    def has_text_files(self) -> bool:
        """æ˜¯å¦æœ‰æ–‡æœ¬æ–‡ä»¶"""
        return len(self.text_files) > 0


class DownloadService:
    """ä¸‹è½½æœåŠ¡ç±»"""
    
    def __init__(self, config_manager: ConfigManager, logger: Logger, 
                 network_utils: NetworkUtils, file_utils: FileUtils):
        """
        åˆå§‹åŒ–ä¸‹è½½æœåŠ¡
        
        Args:
            config_manager: é…ç½®ç®¡ç†å™¨
            logger: æ—¥å¿—è®°å½•å™¨
            network_utils: ç½‘ç»œå·¥å…·
            file_utils: æ–‡ä»¶å·¥å…·
        """
        self.config_manager = config_manager
        self.logger = logger
        self.network_utils = network_utils
        self.file_utils = file_utils
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        self.temp_dir = Path("temp")
        self.file_utils.ensure_dir(self.temp_dir)
        
        # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
        cache_info = self.network_utils.get_cache_info()
        if cache_info['total_files'] > 0:
            self.logger.info(f"ğŸ’¾ ç¼“å­˜çŠ¶æ€: {cache_info['total_files']} ä¸ªæ–‡ä»¶, "
                           f"{cache_info['total_size_mb']:.2f} MB")
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        cleared = self.network_utils.clear_cache(older_than_hours=48)  # æ¸…ç†48å°æ—¶å‰çš„ç¼“å­˜
        if cleared > 0:
            self.logger.info(f"ğŸ§¹ å·²æ¸…ç† {cleared} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")
    
    def is_json_ruleset(self, url: str) -> bool:
        """
        æ£€æŸ¥URLæ˜¯å¦ä¸ºJSONè§„åˆ™é›†
        
        Args:
            url: è¦æ£€æŸ¥çš„URL
            
        Returns:
            æ˜¯å¦ä¸ºJSONè§„åˆ™é›†
        """
        return self.network_utils.is_json_url(url)
    
    def download_json_rulesets(self, urls: List[str]) -> List[Dict[str, Any]]:
        """
        ä¸‹è½½JSONè§„åˆ™é›†åˆ—è¡¨
        
        Args:
            urls: JSONè§„åˆ™é›†URLåˆ—è¡¨
            
        Returns:
            æˆåŠŸä¸‹è½½çš„JSONæ•°æ®åˆ—è¡¨
        """
        json_data_list = []
        
        for i, url in enumerate(urls, 1):
            self.logger.info(f"ä¸‹è½½JSONè§„åˆ™é›† ({i}/{len(urls)}): {url}")
            
            json_data = self.network_utils.download_json(url)
            if json_data:
                json_data_list.append(json_data)
                self.logger.info(f"âœ… JSONè§„åˆ™é›†ä¸‹è½½æˆåŠŸ")
            else:
                self.logger.warning(f"âš ï¸ JSONè§„åˆ™é›†ä¸‹è½½å¤±è´¥: {url}")
        
        return json_data_list
    
    def download_text_rulesets(self, urls: List[str], temp_dir: Path) -> List[str]:
        """
        ä¸‹è½½æ–‡æœ¬è§„åˆ™é›†åˆ—è¡¨ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å¹¶å‘ä¸‹è½½
        
        Args:
            urls: æ–‡æœ¬è§„åˆ™é›†URLåˆ—è¡¨
            temp_dir: ä¸´æ—¶ç›®å½•
            
        Returns:
            æˆåŠŸä¸‹è½½çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # å‡†å¤‡ä¸‹è½½ä»»åŠ¡
        download_tasks = []
        for i, url in enumerate(urls, 1):
            filename = self.network_utils.get_filename_from_url(url)
            if not filename.endswith('.txt'):
                filename = f"file_{i}.txt"
            
            output_path = temp_dir / filename
            download_tasks.append((url, output_path))
        
        # å¹¶å‘ä¸‹è½½with enhanced progress and speed display
        self.logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘ä¸‹è½½ {len(urls)} ä¸ªæ–‡æœ¬æ–‡ä»¶ (å¹¶å‘æ•°: {self.network_utils.max_concurrent})")
        
        # è¿›åº¦è·Ÿè¸ªå˜é‡
        last_progress_time = time.time()
        
        def progress_callback(completed: int, total: int, current_file: str, speed_mbps: float, elapsed_time: float):
            """å¢å¼ºçš„è¿›åº¦å›è°ƒï¼Œæ˜¾ç¤ºé€Ÿåº¦å’Œç»Ÿè®¡ä¿¡æ¯"""
            nonlocal last_progress_time
            current_time = time.time()
            
            # é™åˆ¶è¿›åº¦æ›´æ–°é¢‘ç‡ï¼ˆæ¯0.5ç§’æ›´æ–°ä¸€æ¬¡ï¼‰
            if current_time - last_progress_time >= 0.5 or completed == total:
                last_progress_time = current_time
                
                # æ„å»ºè¿›åº¦æ¶ˆæ¯
                if speed_mbps > 0:
                    speed_text = f"é€Ÿåº¦: {speed_mbps:.2f} MB/s"
                else:
                    speed_text = "è®¡ç®—é€Ÿåº¦ä¸­..."
                
                time_text = f"å·²ç”¨æ—¶: {elapsed_time:.1f}s"
                progress_msg = f"{speed_text}, {time_text}"
                
                self.logger.progress(completed, total, progress_msg)
        
        # ä½¿ç”¨å¢å¼ºçš„å¹¶å‘ä¸‹è½½
        results, stats = self.network_utils.download_multiple_with_stats(
            download_tasks, 
            max_workers=self.network_utils.max_concurrent
        )
        
        # æ”¶é›†æˆåŠŸä¸‹è½½çš„æ–‡ä»¶
        successful_files = []
        failed_urls = []
        
        for result in results:
            if result.success:
                successful_files.append(result.file_path)
            else:
                failed_urls.append(result.url)
                self.logger.warning(f"âš ï¸ æ–‡ä»¶ä¸‹è½½å¤±è´¥: {result.url} - {result.error}")
        
        # æ˜¾ç¤ºè¯¦ç»†çš„ä¸‹è½½ç»Ÿè®¡
        self.logger.info(f"âœ… æ–‡æœ¬æ–‡ä»¶ä¸‹è½½å®Œæˆ: {stats['successful_files']}/{stats['total_files']} æˆåŠŸ")
        self.logger.info(f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
        self.logger.info(f"   â€¢ æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        self.logger.info(f"   â€¢ æ€»å¤§å°: {stats['total_size_mb']:.2f} MB")
        self.logger.info(f"   â€¢ æ€»è€—æ—¶: {stats['total_time_seconds']:.1f} ç§’")
        self.logger.info(f"   â€¢ å¹³å‡é€Ÿåº¦: {stats['average_speed_mbps']:.2f} MB/s")
        self.logger.info(f"   â€¢ å¹¶å‘æ•°: {stats['max_concurrent']}")
        
        if stats['failed_files'] > 0:
            self.logger.warning(f"âš ï¸ {stats['failed_files']} ä¸ªæ–‡ä»¶ä¸‹è½½å¤±è´¥")
        
        return successful_files
    
    def download_ruleset(self, ruleset_name: str, urls: List[str]) -> DownloadedData:
        """
        ä¸‹è½½å•ä¸ªè§„åˆ™é›†çš„æ‰€æœ‰æ•°æ®æº
        
        Args:
            ruleset_name: è§„åˆ™é›†åç§°
            urls: URLåˆ—è¡¨
            
        Returns:
            ä¸‹è½½æ•°æ®ç»“æœ
        """
        self.logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½è§„åˆ™é›†: {ruleset_name}")
        self.logger.info(f"ğŸ“‹ æ•°æ®æºæ•°é‡: {len(urls)}")
        
        # åˆ›å»ºä¸‹è½½ç»“æœå¯¹è±¡
        downloaded_data = DownloadedData(ruleset_name)
        downloaded_data.set_total_count(len(urls))
        
        # åˆ†ç±»URL
        json_urls = [url for url in urls if self.is_json_ruleset(url)]
        text_urls = [url for url in urls if not self.is_json_ruleset(url)]
        
        self.logger.info(f"ğŸ“Š JSONè§„åˆ™é›†: {len(json_urls)} ä¸ª, æ–‡æœ¬è§„åˆ™é›†: {len(text_urls)} ä¸ª")
        
        # ä¸‹è½½JSONè§„åˆ™é›†
        if json_urls:
            self.logger.info(f"ğŸ”„ å¼€å§‹ä¸‹è½½JSONè§„åˆ™é›†")
            json_data_list = self.download_json_rulesets(json_urls)
            
            for json_data in json_data_list:
                downloaded_data.add_json_data(json_data)
            
            if len(json_data_list) != len(json_urls):
                failed_json = len(json_urls) - len(json_data_list)
                downloaded_data.add_error(f"{failed_json} ä¸ªJSONè§„åˆ™é›†ä¸‹è½½å¤±è´¥")
        
        # ä¸‹è½½æ–‡æœ¬è§„åˆ™é›†
        if text_urls:
            self.logger.info(f"ğŸ”„ å¼€å§‹ä¸‹è½½æ–‡æœ¬è§„åˆ™é›†")
            
            # ä¸ºæ¯ä¸ªè§„åˆ™é›†åˆ›å»ºç‹¬ç«‹çš„ä¸´æ—¶ç›®å½•
            ruleset_temp_dir = self.temp_dir / ruleset_name
            self.file_utils.ensure_dir(ruleset_temp_dir)
            
            text_files = self.download_text_rulesets(text_urls, ruleset_temp_dir)
            
            for file_path in text_files:
                downloaded_data.add_text_file(file_path)
            
            if len(text_files) != len(text_urls):
                failed_text = len(text_urls) - len(text_files)
                downloaded_data.add_error(f"{failed_text} ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸‹è½½å¤±è´¥")
        
        # è¾“å‡ºä¸‹è½½ç»“æœæ‘˜è¦
        if downloaded_data.is_successful():
            self.logger.info(f"âœ… è§„åˆ™é›† {ruleset_name} ä¸‹è½½å®Œæˆ")
            self.logger.info(f"ğŸ“Š æˆåŠŸ: {downloaded_data.success_count}/{downloaded_data.total_count}")
            
            if downloaded_data.has_json_data():
                self.logger.info(f"ğŸ“„ JSONæ•°æ®: {len(downloaded_data.json_data)} ä¸ª")
            
            if downloaded_data.has_text_files():
                self.logger.info(f"ğŸ“„ æ–‡æœ¬æ–‡ä»¶: {len(downloaded_data.text_files)} ä¸ª")
        else:
            self.logger.error(f"âŒ è§„åˆ™é›† {ruleset_name} ä¸‹è½½å¤±è´¥")
        
        # è¾“å‡ºé”™è¯¯ä¿¡æ¯
        for error in downloaded_data.errors:
            self.logger.warning(f"âš ï¸ {error}")
        
        return downloaded_data
    
    def download_all_rulesets(self) -> Dict[str, DownloadedData]:
        """
        ä¸‹è½½æ‰€æœ‰è§„åˆ™é›†
        
        Returns:
            è§„åˆ™é›†åç§°åˆ°ä¸‹è½½æ•°æ®çš„æ˜ å°„
        """
        rulesets = self.config_manager.get_rulesets()
        results = {}
        
        self.logger.header("å¼€å§‹ä¸‹è½½é˜¶æ®µ")
        self.logger.info(f"ğŸ“‹ å‘ç° {len(rulesets)} ä¸ªè§„åˆ™é›†")
        
        for i, (ruleset_name, urls) in enumerate(rulesets.items(), 1):
            self.logger.step(f"ä¸‹è½½è§„åˆ™é›†: {ruleset_name}", i, len(rulesets))
            
            try:
                downloaded_data = self.download_ruleset(ruleset_name, urls)
                results[ruleset_name] = downloaded_data
                
            except Exception as e:
                self.logger.error(f"âŒ è§„åˆ™é›† {ruleset_name} ä¸‹è½½å¼‚å¸¸: {str(e)}")
                # åˆ›å»ºå¤±è´¥çš„ä¸‹è½½æ•°æ®
                failed_data = DownloadedData(ruleset_name)
                failed_data.set_total_count(len(urls))
                failed_data.add_error(f"ä¸‹è½½å¼‚å¸¸: {str(e)}")
                results[ruleset_name] = failed_data
            
            # æ·»åŠ åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
            if i < len(rulesets):
                self.logger.info("â”€" * 50)
        
        # è¾“å‡ºæ€»ä½“ç»Ÿè®¡
        successful_rulesets = sum(1 for data in results.values() if data.is_successful())
        self.logger.separator("ä¸‹è½½é˜¶æ®µå®Œæˆ")
        self.logger.success(f"âœ… ä¸‹è½½å®Œæˆ: {successful_rulesets}/{len(rulesets)} ä¸ªè§„åˆ™é›†æˆåŠŸ")
        
        return results
    
    def cleanup_temp_files(self, keep_patterns: Optional[List[str]] = None) -> None:
        """
        æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        
        Args:
            keep_patterns: è¦ä¿ç•™çš„æ–‡ä»¶æ¨¡å¼åˆ—è¡¨
        """
        if self.temp_dir.exists():
            deleted_count = self.file_utils.cleanup_temp_files(self.temp_dir, keep_patterns)
            if deleted_count > 0:
                self.logger.info(f"ğŸ§¹ å·²æ¸…ç† {deleted_count} ä¸ªä¸´æ—¶æ–‡ä»¶")
    
    def get_download_statistics(self, results: Dict[str, DownloadedData]) -> Dict[str, Any]:
        """
        è·å–ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            results: ä¸‹è½½ç»“æœå­—å…¸
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total_rulesets = len(results)
        successful_rulesets = sum(1 for data in results.values() if data.is_successful())
        total_sources = sum(data.total_count for data in results.values())
        successful_sources = sum(data.success_count for data in results.values())
        
        json_rulesets = sum(1 for data in results.values() if data.has_json_data())
        text_rulesets = sum(1 for data in results.values() if data.has_text_files())
        
        return {
            'total_rulesets': total_rulesets,
            'successful_rulesets': successful_rulesets,
            'total_sources': total_sources,
            'successful_sources': successful_sources,
            'json_rulesets': json_rulesets,
            'text_rulesets': text_rulesets,
            'success_rate': (successful_sources / total_sources * 100) if total_sources > 0 else 0
        }